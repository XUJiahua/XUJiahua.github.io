<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 许嘉华的博客</title>
    <link>https://xujiahua.github.io/posts/</link>
    <description>Recent content in Posts on 许嘉华的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 21 Apr 2020 09:01:38 +0800</lastBuildDate>
    
	<atom:link href="https://xujiahua.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>consul 小结</title>
      <link>https://xujiahua.github.io/posts/20200421-use-consul/</link>
      <pubDate>Tue, 21 Apr 2020 09:01:38 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200421-use-consul/</guid>
      <description>简单介绍 hashicorp 对 Consul 的定位是服务间网络方案。
 Consul is a service networking solution to connect and secure services across any runtime platform and public or private cloud. https://www.consul.io/
 官方的两个 use case 就是 service discovery, service mesh。
service discovery 与 etcd 比起来，Consul 的服务发现是开箱即用的。优点如下：
 First class 的服务注册和获取接口。不需要像 etcd 那样在 kv 存储基础上做包装。 服务注册，可以是consul 命令，也可以是HTTP API。 获取服务注册表，除了 HTTP 接口，还可以使用 DNS 查询接口。 健康检查。服务注册的时候可以提供健康检查项，客户端获取服务注册表时consul会过滤掉不健康的service。不担心拿到“脏”服务。健康检查也包括节点的检查。健康检查这个功能，就是一个分布式监控工具。 Web 管理界面。节点、服务、健康与否一目了然。 Watch 功能。通过 blocking queries/ long-polling HTTP API 的方式得到服务注册表的改变的通知。 跨数据中心（取资源）。When a request is made for a resource in another datacenter, the local Consul servers forward an RPC request to the remote Consul servers for that resource and return the results.</description>
    </item>
    
    <item>
      <title>etcd 小结</title>
      <link>https://xujiahua.github.io/posts/20200420-use_etcd/</link>
      <pubDate>Mon, 20 Apr 2020 14:12:38 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200420-use_etcd/</guid>
      <description>简介 取名有意思。Linux下的/etc目录放的是配置文件。etcd，etc代表配置，d代表distributed，代表分布式配置。
特点：
 designed to reliably store infrequently updated data and provide reliable watch queries https://etcd.io/docs/v3.4.0/learning/data_model/ KV 核心用户接口 MVCC Multi-version Concurrency Control 也确实能读历史版本 Raft consensus algorithms 共识算法 Watch 配置更新能及时&amp;quot;通知&amp;quot;应用 RBAC 用户、角色、权限  基于 etcd 可以做哪些事情：
 配置中心。元数据存储。应用的配置集中存储在配置中心。 服务发现。配置中心的一个特例。相比起来，consul的服务发现是开箱即用的。 分布式锁。分布式系统协调。选主。像是Hadoop使用Zookeeper做Namenode的选主。  vs. Consul。Consul 官方（https://www.consul.io/）定义的usecase是 service discovery和 service mesh。
 etcd and Consul solve different problems. If looking for a distributed consistent key value store, etcd is a better choice over Consul.</description>
    </item>
    
    <item>
      <title>k8s configmap 与热更新</title>
      <link>https://xujiahua.github.io/posts/20200417-kubernetes-configmap/</link>
      <pubDate>Fri, 17 Apr 2020 16:09:28 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200417-kubernetes-configmap/</guid>
      <description>configmap 简介 官方介绍：使用 ConfigMap 配置 Pod https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-pod-configmap/
他人总结：ConfigMap https://jimmysong.io/kubernetes-handbook/concepts/configmap.html
稍微总结下：
 每个configmap都有一个名字，名字全局唯一（命名空间内），重复创建会报错。 每个configmap本身是键值对。 configmap可以通过环境变量的方式让Pod内容器读取。 configmap可以通过挂载文件的方式让Pod内容器读取。k8s每隔一段时间同步configmap，如果有更新的话。当然，应用本身是不知道的。这个定时更新感觉有点鸡肋。 configmap更新，不会自动重启应用。只能人工方式，滚动重启应用。  把配置更新也当作一次应用变更看待，心情就好很多了。
官方不支持热更新，所以有了各种技巧，提高效率。
 create a new ConfigMap with the changes you want to make, and point your deployment at the new ConfigMap https://stackoverflow.com/a/40624029/820682 因为 deployment 文件变化了，触发滚动重启。 还有deployment 文件中配置 configmap hash值的。配置变化，hash值变化，deployment变化，滚动重启，一级级联动。 https://blog.questionable.services/article/kubernetes-deployments-configmap-change/ 还有使用sidecar的方式做热更新的，太复杂了 https://zhuanlan.zhihu.com/p/57570231  关于热更新 configmap的更新，容器化应用是无感知的。configmap这种方式没有推送更新到应用内的机制，要实现热更新过于复杂。
k8s最核心的功能还是自动部署、伸缩、容器管理以及资源分配。微服务架构还是得需要其他框架来辅助的。
配置热更新应用，就选择 etcd, consul 吧，有 watch 功能。</description>
    </item>
    
    <item>
      <title>etcd api client 请求重试逻辑</title>
      <link>https://xujiahua.github.io/posts/20200417-etcd_client/</link>
      <pubDate>Fri, 17 Apr 2020 15:05:51 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200417-etcd_client/</guid>
      <description>使用 Consul 作为配置中心，按照官方的说法，没必要创建 consul client 节点。那么直接连 consul server 就好了。
 Running an agent is not required for discovering other services or getting/setting key/value data. The agent is responsible for health checking the services on the node as well as the node itself.
https://www.consul.io/intro/index.html#basic-architecture-of-consul
 Consul api client (https://github.com/hashicorp/consul/tree/master/api) 目前只能接收一个server地址。那么这个server地址得保证高可用才行啊。
etcd api client (https://github.com/etcd-io/etcd/tree/master/client) 倒是能接收多个server地址，看看 etcd 是怎么做的。
etcd api client 创建了 httpClusterClient。
多个 endpoint 的处理核心逻辑。
 pinned 用于记录好用的连接地址的index，优先使用这个地址。 context 类错误，比如取消请求，直接退出。 遇到 5xx 类错误，服务端错误。需要考虑是否重试了。 isOneShot 标记，true 代表是 Set/Delete 操作，请求失败不再重试。应该跟请求是否幂等有关。 可以重试的请求，重试直到成功或是循环结束。  </description>
    </item>
    
    <item>
      <title>机器学习在线推理部署方案：Cortex</title>
      <link>https://xujiahua.github.io/posts/20200416-cortex/</link>
      <pubDate>Thu, 16 Apr 2020 11:33:18 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200416-cortex/</guid>
      <description>Cortex 介绍 官方网站：Deploy machine learning models in production https://www.cortex.dev/
GitHub https://github.com/cortexlabs/cortex
 The CLI sends configuration and code to the cluster every time you run cortex deploy. Each model is loaded into a Docker container, along with any Python packages and request handling code. The model is exposed as a web service using Elastic Load Balancing (ELB), TensorFlow Serving, and ONNX Runtime. The containers are orchestrated on Elastic Kubernetes Service (EKS) while logs and metrics are streamed to CloudWatch.</description>
    </item>
    
    <item>
      <title>k8s 应用日志收集</title>
      <link>https://xujiahua.github.io/posts/20200414-k8s-logging/</link>
      <pubDate>Tue, 14 Apr 2020 09:53:35 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200414-k8s-logging/</guid>
      <description>k8s 日志收集架构 以下是比较一般、普适的架构。更多参考：Kubernetes 日志架构 https://kubernetes.io/zh/docs/concepts/cluster-administration/logging/
 容器化应用将日志写入stdout、stderr。 Docker容器引擎将stdout、stderr流重定向到日志驱动，比如默认的json-file。 json-file日志驱动将日志写入到（宿主机上的）文件。 日志收集工具以DaemonSet的形式安装在每个节点。 日志收集工具监听文件变化，并将日志写入到日志中心服务。  k8s 日志收集细节 实战 可以直接参考以下教程：minikube创建了一个Kubernetes集群，Fluentd收集日志，存入ElasticSearch，使用Kibana查看日志。典型的EFK技术栈。
 Logging in Kubernetes with Elasticsearch, Kibana, and Fluentd https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/  在Kibana上看收集到的日志。能看到日志收集工具也采集了容器、镜像、Pod有关的信息。这些上下文信息能让人定位到是哪个应用在生产日志。
fluentd 收集上下文信息 Docker json-file 日志驱动写文件，并不记录上下文信息。 https://docs.docker.com/config/containers/logging/json-file/
{&amp;quot;log&amp;quot;:&amp;quot;Log line is here\n&amp;quot;,&amp;quot;stream&amp;quot;:&amp;quot;stdout&amp;quot;,&amp;quot;time&amp;quot;:&amp;quot;2019-01-01T11:11:11.111111111Z&amp;quot;} 上文中使用的日志收集镜像是 fluent/fluentd-kubernetes-daemonset:v1.3-debian-elasticsearch。
具体代码路径在此 https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.3/debian-elasticsearch
收集容器目录下的日志。
使用kubernetes_metadata这个第三方插件获取容器相关的上下文信息。这里是通过请求API server得到metadata的。
kubernetes_metadata 插件地址 https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter
插件中有缓存metadata的选项，不用担心每处理一条日志，就要向API server发送请求。</description>
    </item>
    
    <item>
      <title>Metabase &#43; Spark SQL</title>
      <link>https://xujiahua.github.io/posts/20200410-metabase-spark-sql/</link>
      <pubDate>Fri, 10 Apr 2020 16:41:53 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200410-metabase-spark-sql/</guid>
      <description>这是大数据 BI 平台的第二步，BI 工具的搭建。假设已经配置好 Spark SQL JDBC Server，并启用了Kerberos。参考 https://xujiahua.github.io/posts/20200410-spark-thrift-server-cdh/
这里，我们选择了开源产品 Metabase。
最终，大数据 BI 平台，是由 1) 以Metabase作为BI可视化，2) 由HDFS（分布式文件存储） + parquet（列式数据存储格式）+ Hive metastore（SQL表结构信息维护） + Spark SQL（批处理引擎）组合的OLAP数据库组成。
Metabase 简介 Metabase is the easy, open source way for everyone in your company to ask questions and learn from data.
https://www.metabase.com/
数据库支持  BigQuery Druid Google Analytics H2 MongoDB MySQL/MariaDB PostgreSQL Presto Amazon Redshift Snowflake Spark SQL SQLite SQL Server  https://www.metabase.com/docs/latest/faq/setup/which-databases-does-metabase-support.html
这里有我们需要的Spark SQL，我们的大数据集群可以支持。比较遗憾的是没有Impala。
Metabase 安装 MySQL 使用MySQL作为元数据存储。复用之前CDH的MySQL实例。</description>
    </item>
    
    <item>
      <title>CDH6 启用 Spark Thrift Server</title>
      <link>https://xujiahua.github.io/posts/20200410-spark-thrift-server-cdh/</link>
      <pubDate>Fri, 10 Apr 2020 10:07:16 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200410-spark-thrift-server-cdh/</guid>
      <description>很遗憾，CDH版本的Spark阉割了Thrift Server。（可能与自家产品Impala有竞争关系的原因。）
参考 https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/spark.html#spark__d99299e107
# ll /opt/cloudera/parcels/CDH/lib/spark/sbin/ total 84 -rwxr-xr-x 1 root root 2803 Nov 9 00:05 slaves.sh -rwxr-xr-x 1 root root 1429 Nov 9 00:05 spark-config.sh -rwxr-xr-x 1 root root 5689 Nov 9 00:05 spark-daemon.sh -rwxr-xr-x 1 root root 1262 Nov 9 00:05 spark-daemons.sh -rwxr-xr-x 1 root root 1190 Nov 9 00:05 start-all.sh -rwxr-xr-x 1 root root 1274 Nov 9 00:05 start-history-server.sh -rwxr-xr-x 1 root root 2050 Nov 9 00:05 start-master.</description>
    </item>
    
    <item>
      <title>Docker日志驱动小结</title>
      <link>https://xujiahua.github.io/posts/20200403-docker-logging/</link>
      <pubDate>Fri, 03 Apr 2020 15:43:07 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200403-docker-logging/</guid>
      <description>docker logs， kubectl logs能看到Docker容器的标准输出、标准错误，方便定位问题。而 xxx logs之所以能看到，是因为标准输出、标准错误存储在每个容器独有的日志文件中。
另外日志量大了，用docker logs看历史数据不大合适。我们就需要考虑将日志存储到日志中心去。
Docker默认支持如下日志驱动。有直接写文件的，有使用云服务的。下面简单介绍下。
credit: https://jaxenter.com/docker-logging-gotchas-137049.html
官方文档 https://docs.docker.com/config/containers/logging/configure/
默认驱动：json-file 默认的Logging Driver是json-file。docker info可以查看。全局的日志驱动设置，可以修改daemon配置文件 /etc/docker/daemon.json。
写入文件的日志格式长这样：{&amp;quot;log&amp;quot;:&amp;quot;Log line is here\n&amp;quot;,&amp;quot;stream&amp;quot;:&amp;quot;stdout&amp;quot;,&amp;quot;time&amp;quot;:&amp;quot;2019-01-01T11:11:11.111111111Z&amp;quot;}，每一行是一个json文件，log字段为容器原来输出的每行内容。
默认配置，创建的容器的信息在这个目录下： /var/lib/docker/containers。
实验 root@ubuntu-parallel:~# docker run --name default_logging_driver hello-world root@ubuntu-parallel:~# cd /var/lib/docker/containers/$(docker ps --no-trunc -aqf &amp;#34;name=default_logging_driver&amp;#34;) root@ubuntu-parallel:~# cat $(docker ps --no-trunc -aqf &amp;#34;name=default_logging_driver&amp;#34;)-json.log {&amp;#34;log&amp;#34;:&amp;#34;\n&amp;#34;,&amp;#34;stream&amp;#34;:&amp;#34;stdout&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2020-04-02T01:46:54.096347888Z&amp;#34;} {&amp;#34;log&amp;#34;:&amp;#34;Hello from Docker!\n&amp;#34;,&amp;#34;stream&amp;#34;:&amp;#34;stdout&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2020-04-02T01:46:54.096377382Z&amp;#34;} {&amp;#34;log&amp;#34;:&amp;#34;This message shows that your installation appears to be working correctly.\n&amp;#34;,&amp;#34;stream&amp;#34;:&amp;#34;stdout&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2020-04-02T01:46:54.096381118Z&amp;#34;} {&amp;#34;log&amp;#34;:&amp;#34;\n&amp;#34;,&amp;#34;stream&amp;#34;:&amp;#34;stdout&amp;#34;,&amp;#34;time&amp;#34;:&amp;#34;2020-04-02T01:46:54.096383725Z&amp;#34;} https://docs.docker.com/config/containers/logging/json-file/
怎么记录更多上下文信息 json-file本身是没有记录上下文信息的。集中存储到日志中心服务器，就无法区分具体是哪个应用产生的日志了。
k8s的容器日志收集，上下文信息是由日志收集工具 fluentd 通过请求api server采集并缓存起来的。参考 https://xujiahua.github.io/posts/20200414-k8s-logging/
同样的思路，fluentd也有不少通过docker daemon查询或是解析容器目录下config.</description>
    </item>
    
    <item>
      <title>Fluentd实战</title>
      <link>https://xujiahua.github.io/posts/20200402-use-fluentd/</link>
      <pubDate>Thu, 02 Apr 2020 16:58:23 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200402-use-fluentd/</guid>
      <description>以收集Docker容器日志的例子，介绍下Fluentd的用法。不考虑logstash，太占服务器资源了。
安装 Fluentd Ubuntu 18.04上的安装命令（https://docs.fluentd.org/installation/install-by-deb）：
root@ubuntu-parallel:~# curl -L https://toolbelt.treasuredata.com/sh/install-ubuntu-bionic-td-agent3.sh | sh 以Daemon方式启动：
root@ubuntu-parallel:~# systemctl start td-agent.service root@ubuntu-parallel:~# systemctl status td-agent.service fluentd的安装目录是在/opt/td-agent/下的。为演示方便，我们可以直接使用 /opt/td-agent/embedded/bin/fluentd这个程序。
root@ubuntu-parallel:~# ps -ef | grep fluentd td-agent 30596 1 0 17:10 ? 00:00:00 /opt/td-agent/embedded/bin/ruby /opt/td-agent/embedded/bin/fluentd --log /var/log/td-agent/td-agent.log --daemon /var/run/td-agent/td-agent.pid td-agent 30602 30596 9 17:10 ? 00:00:00 /opt/td-agent/embedded/bin/ruby -Eascii-8bit:ascii-8bit /opt/td-agent/embedded/bin/fluentd --log /var/log/td-agent/td-agent.log --daemon /var/run/td-agent/td-agent.pid --under-supervisor 其他系统的安装参考：https://docs.fluentd.org/installation
小试牛刀 配置文件 test.conf，启动一个HTTP服务，并把接收到的日志，打印到标准输出。
&amp;lt;source&amp;gt; @type http port 9880 &amp;lt;/source&amp;gt; &amp;lt;match *.*&amp;gt; @type stdout &amp;lt;/match&amp;gt; 启动fluentd进程。</description>
    </item>
    
    <item>
      <title>Google Vision API</title>
      <link>https://xujiahua.github.io/posts/20200401-google-vision-api/</link>
      <pubDate>Wed, 01 Apr 2020 14:20:51 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200401-google-vision-api/</guid>
      <description>目标 业务上需要识别出文本、图像敏感内容，降低业务风险。
调用下云服务的产品。
Google Cloud Platform GCP上，图像识别有两个产品：
 Vision API 直接使用预先训练的模型 AutoML 迁移学习，使用用户提交的新分类数据，训练模型。  关于文本识别的产品是类似的，也分直接使用预训练模型和迁移学习重新训练。
Vision API：图像内容识别  Google Vision API的模型已经有审核内容的能力：暴力、成人内容的识别。 提供API，也提供了各个语言的SDK。 不需要开发者训练模型。  参考：https://cloud.google.com/vision?hl=zh-cn
试用 安全搜索的分类说明  Adult 成人内容 Spoof 恶搞，比如恶搞政治人物 Medical 医疗影像 Violence 暴力内容 Racy 猥亵类，类似成人内容  参考：https://cloud.google.com/vision/docs/reference/rpc/google.cloud.vision.v1?hl=zh-cn#google.cloud.vision.v1.SafeSearchAnnotation
对接流程 参考：https://codelabs.developers.google.com/codelabs/cloud-vision-intro/index.html?index=..%2F..cloudai&amp;amp;hl=zh-cn&amp;amp;_ga=2.102874504.925825070.1585721382-1910642988.1585296097#0
注意，图像不一定需要上传到Google的云存储。
开发对接参考：检测露骨内容（安全搜索）https://cloud.google.com/vision/docs/detecting-safe-search?hl=zh-cn
DEMO 创建服务账号、创建密钥，自动下载json文件：
code：
https://github.com/GoogleCloudPlatform/golang-samples/blob/master/vision/detect/README.md
请求次数限制 对请求内容大小有限制外，需要注意请求配额，每分钟请求数1800，QPS也就是30。如果无法满足需求，需要在平台申请增加配额。
使用默认配额，业务上需要考虑采样数据，再调用Vision API。
参考 https://cloud.google.com/vision/quotas?hl=zh-cn
请求响应时间 请求响应时间，取决于两个因素：
上传数据大小 越大越慢。
 手机拍照软件生成的图像一般3M，建议图像压缩后上传。 考虑gcloud上先存储图像文件。参考 https://medium.com/bankify-tech-blog/how-to-optimize-the-speed-of-google-vision-api-cdc5e452104b  Vision API本身 对一张249K的图像进行简单压测。
$ ll demo/trump.jpg -rw-r--r-- 1 jiahua staff 249K Apr 1 14:14 demo/trump.</description>
    </item>
    
    <item>
      <title>使用 Docker Machine</title>
      <link>https://xujiahua.github.io/posts/20200331-use-docker-machine/</link>
      <pubDate>Tue, 31 Mar 2020 09:53:05 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200331-use-docker-machine/</guid>
      <description>简介 Docker Machine lets you create Docker hosts on your computer, on cloud providers, and inside your own data center. It creates servers, installs Docker on them, then configures the Docker client to talk to them.
参考：
 https://github.com/docker/machine https://docs.docker.com/machine/overview/  vs. Vagrant 与Vagrant的交集 目前使用 Vagrant 搭 Docker 环境的步骤如下：
 vagrant init {box_name}，下载一个基础的虚拟机镜像，比如centos，并创建一个Vagrantfile。 Vagrantfile中设置虚拟机hostname。 Vagrantfile中设置Private network，使得几个VM可以互相通信。 VM内安装docker软件。  如果用上了docker-machine，只要一行命令。在这个场景上，docker-machine 比 Vagrant 方便了很多。
docker-machine create -d virtualbox {host_name} VM的管理，vagrant命令需要在Vagrantfile所在目录执行。而docker-machine可以在任何目录管理VM。
其他特色  通过其他driver，可以安装管理云主机或是私有数据中心，而不仅仅是virtualbox。 通过eval &amp;ldquo;$(docker-machine env default)&amp;quot;，覆盖环境变量DOCKER_HOST，使得本地docker client访问VM内的docker daemon。 更多实用命令，参考 https://docs.</description>
    </item>
    
    <item>
      <title>Docker Network小结</title>
      <link>https://xujiahua.github.io/posts/20200321-docker-network/</link>
      <pubDate>Sat, 21 Mar 2020 14:10:40 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200321-docker-network/</guid>
      <description>Docker网络非常值得学习。对Docker不熟的同学，建议先看一些入门资料。
Docker 学习资料  Docker — 从入门到实践 https://www.yuque.com/grasilife/docker Docker Kubernetes Lab Handbook https://docker-k8s-lab.readthedocs.io/en/latest/index.html 「Docker进阶与实战」华为Docker实践小组  单机网络 （建议在Linux系统上实验。）
Docker安装完后，默认有三个（三种）网络。分别是默认的bridge模式，host模式，none模式。
# docker network ls NETWORK ID NAME DRIVER SCOPE 51cbe7a9bb19 bridge bridge local 7182ef9fa8c4 host host local bd80d0dedaa8 none null local 参考：https://docs.docker.com/network/
none模式 &amp;ndash;net=none 无网络。
# docker run --net=none --rm -it alpine ip addr show 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever host模式 &amp;ndash;net=host  容器共用宿主机的网络。但是，容器的其他方面，如文件系统、进程列表等还是和宿主机隔离的。 容器不会申请独立的IP。 容器申请的端口占用宿主机的端口资源。 -p等端口映射命令不起作用：WARNING: Published ports are discarded when using host network mode 性能上比较好，因为没有地址转换。Host mode networking can be useful to optimize performance, and in situations where a container needs to handle a large range of ports, as it does not require network address translation (NAT), and no “userland-proxy” is created for each port.</description>
    </item>
    
    <item>
      <title>Go Web 项目框架</title>
      <link>https://xujiahua.github.io/posts/20200320-go-web-project/</link>
      <pubDate>Fri, 20 Mar 2020 15:24:22 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200320-go-web-project/</guid>
      <description>常用开源库 依赖包管理 Go Modules Go Modules。Go 从1.11版本开始支持，Go 1.14被认为是生产可用了。个人用过最方便的Go依赖包管理工具了。
版本管理中维护 go.mod/go.sum 两个文件。
参考  Using Go Modules https://blog.golang.org/using-go-modules  命令行框架 Corba Corba。第一次看到，是在翻看 Hyperledger Fabric 的时候。
 子命令，嵌套的子命令。 增强版本的flags。 项目模板的生成工具。 生成工具也自动引入了Viper这个包。Corba与Viper由一个作者开发，两个项目配合使用非常方便。  建议使用Cobra命令行工具生成项目模板。
 ▾ appName/ ▾ cmd/ add.go your.go commands.go here.go main.go 参考  A Commander for modern Go CLI interactions https://github.com/spf13/cobra Cobra Generator https://github.com/spf13/cobra/blob/master/cobra/README.md  配置管理 Viper Viper。常常与Corba搭配使用。抄一段官方简介：
Viper is a complete configuration solution for Go applications including 12-Factor apps. It is designed to work within an application, and can handle all types of configuration needs and formats.</description>
    </item>
    
    <item>
      <title>Go库对URL Path中%2F的处理</title>
      <link>https://xujiahua.github.io/posts/20200312-slash-in-url-path/</link>
      <pubDate>Thu, 12 Mar 2020 15:17:36 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200312-slash-in-url-path/</guid>
      <description>问题描述  有同事反馈Go项目的接口404了。看了下nginx日志，只有部分请求404了，404请求的显著特征是URL里有%2F，也就是/的转义。 接口定义是这样的：/api/xxx/{mid}/{uid}。mid、uid是URL path的一部分。从现象来看，程序没处理好转义字符，因为多了一个path部分，路由不匹配了，404。 nginx的URL记录是%2F，其实调用方传的是微信ID，比如IEd5W/jqsdF9qpuagQscEg==。调用方在发请求之前对ID已经做好了转义。  Go内置库：不区分%2F和/ An application cannot distinguish between &amp;ldquo;/&amp;rdquo; used as a path segment delimiter and &amp;ldquo;/&amp;rdquo; encoded in a path segment.
这个问题由来已久了，看着官方也不打算解决了，见这个issue：https://github.com/golang/go/issues/3659
If your app needs to assign special meaning, the server can reprocess req.RequestURI as it sees fit, and the client can issue requests using &amp;amp;url.URL{Opaque: rawURI}.  reprocess req.RequestURI 就是让你别用内置路由库了。 issue requests using &amp;amp;url.URL{Opaque: rawURI}. 试了，问题没解决。客户端不管怎么做，最终都是以HTTP协议输出的。GET http://www.google.com/index.html HTTP/1.1 服务端代码最终解析的还是中间的那段URL文本。照理就不通啊。  源码角度 url.URL的定义，明确指出Path字段存储的是decode之后的数据，所以%2F在这里已经被转义成了/，跟之前的Path意义完全不同了。</description>
    </item>
    
    <item>
      <title>专利申请流程小结</title>
      <link>https://xujiahua.github.io/posts/20200311-patent/</link>
      <pubDate>Wed, 11 Mar 2020 11:25:09 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200311-patent/</guid>
      <description>去年憋的一篇专利，到申请公布阶段了，算是去年研究区块链的副成果。https://patents.google.com/patent/CN110807211。整理下专利申请流程。
专利申请流程 主要流程如下图，来自网络。
技术成果生成 有可以申请专利的点，这个因人而异。基于点子，写个草稿。
申请专利 找一个专利代理，让他们提供专利稿的修改意见。反复迭代直到专利代理觉得没问题了。
之后专利代理会出具一份「专利申请受理通知书」，你再交点钱，可能还会有税费减免。
我遇到了专利代理看不大懂专利的情况，僵持了一段时间，后面换了一家顺畅好多。
专利申请公布 这个时间点叫做：申请公布日。
说明初审过了。实用新型和外观发明的话，直接就授权了，而发明还需要实质审查。
&amp;ldquo;初步审查合格后，实用新型和外观就直接授权了，而发明还要经过实质审查，检索国内外的资料，确认没有相同的才会授权，所以发明的含金量会高很多，因此发明专利审查的时间也会是比较长的，一般从申请到拿证书都需要2-3年，而实用、外观这两项专利一年能拿到。&amp;rdquo;
专利授权公告 这个时间点叫做：授权公告日。
说明正式审查完了。这时候才算是真正拥有专利。</description>
    </item>
    
    <item>
      <title>Github pages重新开始：使用Hugo、Typora</title>
      <link>https://xujiahua.github.io/posts/20200310-github-pages-restart/</link>
      <pubDate>Tue, 10 Mar 2020 11:24:49 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200310-github-pages-restart/</guid>
      <description>重新开始GitHub pages 曾使用GitHub pages +  jekyll 写过一段时间博客，没啥营养。
打算重新开始：
 把我记录在Evernote和Typora中的资料迁移到GitHub pages上。 另外做一些Web形式的小工具，挂载在一个子目录里，方便平时使用。  Hugo + Typora 打算如下：
 Markdown文件还是使用Typora写。用过MWeb、Typora，Typora使用感受更好些（这么好用还不要钱）。 Hugo作为静态网站生成，创建一个博客模板。  Hugo quickstart # 安装hugo（其实是Go编译的二进制文件） brew install hugo # 创建site hugo new site quickstart # 通过git submodule的方式下载主题 git init git submodule -b master add https://github.com/dillonzq/LoveIt themes/LoveIt # 更新config.toml，配置主题参数，见loveit配置 # 创建博客，创建的posts在content目录下。文件格式为：content/&amp;lt;CATEGORY&amp;gt;/&amp;lt;FILE&amp;gt;.&amp;lt;FORMAT&amp;gt; # hugo new 会采用 archetypes/default.md作为模板，把一些共性配置放在这个模板里吧。 hugo new posts/my-first-post.md # 本地看blog效果，-D表示草稿状态的博客也显示 hugo server -D # 生成静态网站到./public/，也可以指定目录 hugo -D 挑选了一个比较实用的主题 https://hugoloveit.com/</description>
    </item>
    
    <item>
      <title>Hyper-V使用体验</title>
      <link>https://xujiahua.github.io/posts/20200323-use-hyper-v/</link>
      <pubDate>Tue, 03 Mar 2020 22:13:23 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200323-use-hyper-v/</guid>
      <description>Hyper-V 使用体验 Hyper-V是 Windows10 内置的虚拟机软件，没想到我会用到它。
为什么要用Hyper-V  需要使用虚拟机来模拟Kubernetes环境。 Windows Docker依赖Hyper-V。貌似WSL2也是基于Hyper-V。应该是不会卸载Hyper-V了。 Hyper-V与其他虚拟机软件（VirtualBox, VMWare）冲突，用不了其他的，只能用Hyper-V。 Hyper-V与VirtualBox兼容的方案有没有。可能有，不想过于折腾了，那就不得不用。  Hyper-V好不好用  使用体验太差了，从Mac、从其他虚拟机使用过来，默认配置各种踩坑。 安装VM碰到DHCP超时的问题，反复试验无果，重启Windows后VM的DHCP不超时了。总结下，Windows上，如果怎么搞都搞不定，重启下可能就好了。WTF！ 自带的NAT网络有点残疾，VM配置静态IP，网络就不通了。不像VirtualBox是开箱即用的。 最后借助网络上的零碎信息自定义了下NAT网络，各种神奇配置。 Windows还是很屎，除了打游戏，开发体验跟我之前的体验一样糟。最后搞定花了我一天时间。要是用VMware/VirtualBox，可能在2个小时内就能搞定。总结，不好用。  Hyper-V 创建和克隆VM 创建VM 从*.iso文件创建VM。
 注意，Linux VM，不启用Windows Secure Boot。 更多选项里，设置VM的名称和交换机。 体验不好的地方：左上方的「选择操作系统」区域有长时间的加载过程，即使使用本地镜像文件也得等。  克隆VM 从硬盘文件创建VM。
 克隆是为了减少重复配置一些基础环境的工作量。 不像VirtualBox那样有直观的克隆按钮。摸索出的克隆功能是这样的：就是从VM硬盘创建VM。 建议先关机母体VM后，再基于母体VM的硬盘文件克隆。防止母体VM开机状态下一些更新并没有持久化到硬盘文件中。  Hyper-V NAT网络搭建总结 如果使用VMWare/VirtualBox的话，NAT网络是完全自动搭建的。
VMWare中有三种网络模式，Bridging、NAT、Host-only分别对应Hyper-V下的三种交换机，外部、内部、专用。
 Bridging：VM使用物理网络。GUEST VM与HOST相当于通过交换机连上一个网络（细节不确定）。 NAT：VM使用虚拟网络，HOST同时连入物理网络和虚拟网络。VM出口流量都会通过HOST，使用HOST IP。 Host-only：在NAT基础上，VM连不上外网，除非有路由设置。  默认的NAT网络 默认交换机支持NAT，但是瑕疵不少，无法满足工作需要。
Hyper-V Default Switch + VM DHCP 的组合默认有了NAT功能，满足以下：
 虚拟机之间可以互相访问 虚拟机与物理机之间可以互相访问 虚拟机可以访问互联网  但是：
 每次重启，Default Switch的IP都会变，Windows10 物理机上的网卡IPv4配置无法让他不变。 ping baidu.</description>
    </item>
    
    <item>
      <title>使用Go Modules</title>
      <link>https://xujiahua.github.io/posts/20200228-go-modules/</link>
      <pubDate>Fri, 28 Feb 2020 15:05:18 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200228-go-modules/</guid>
      <description>Go Modules使用着感觉不错，Go终于有正常点的依赖管理工具了。记录一二。
使用小结  go.mod, go.sum加入版本管理 Both go.mod and go.sum should be checked into version control. 直接依赖体现在go.mod，间接依赖由直接依赖自己管理，这是个递归的过程 Only direct dependencies are recorded in the go.mod file. go命令自动管理go.mod，这可太方便了 When it encounters an import of a package not provided by any module in go.mod, the go command automatically looks up the module containing that package and adds it to go.mod, using the latest version. 看着比之前的依赖管理好用很多。比如dep。因为网络下载包等问题，最后把依赖包全checkin到vendor目录了。太臃肿。  常用命令  go mod init creates a new module, initializing the go.</description>
    </item>
    
    <item>
      <title>5G认识小结</title>
      <link>https://xujiahua.github.io/posts/20200222-5g/</link>
      <pubDate>Sat, 22 Feb 2020 23:32:14 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200222-5g/</guid>
      <description>去年11月尝鲜，买了个三星5G手机。找基站测5G速度成了刚买时候的乐趣。用了几个月，深感5G功能很鸡肋，5G基站太少。
这两天翻了好多知乎上关于5G的评论，对5G的概念、应用场景有了些厘清。
基本概念  5G，第五代移动通信技术。也就是5G的边界是移动通信网络。 5G的直接作用是提高了移动设备到基站的下载速率，能达到1G/s，也就是所谓的空口延迟低。 5G下载的速度上限还是光纤的下载速度上限。 5G的流量套餐应该不会便宜，除非国家出政策。为什么套餐不会便宜，这得看成本。5G的基站建设成本远比4G高。5G基站覆盖面积小，需要几倍于4G的基站，5G基站耗电成本也是几倍于4G基站。肥了设备商（华为中兴），苦了运营商（移动联通电信），最后还是消费者买单。 理论速度极可能跑不满。使用者多了，平均到每个终端，带宽就不够。现在我的4G速度好慢，完全没到理论速度。现在的矛盾不是需要更快的网络，而是更多的网络。 木桶效应，一个桶能装多少水由短板决定，以前的移动通信是短板，但是通过3G、4G到现在的5G的改善，已经成为长板了。运营商的承载网等配套也得扩容。不然5G的超高下载速率就是一个理论值。又是运营商的成本。  应用场景 大部分人的共识是，5G的杀手级应用仍在探索中。
什么样的场景，只有通过5G能够做到，而其他技术不行，这样的场景就是典型的5G应用场景。要找到技术的适用场景，不能生搬硬套。区块链也是这样一个典型。过滤那些“可以使用XXX技术，但是完全没必要”的场景。
媒体上关于5G的一些应用场景：
 远程手术 —— 为什么不用光纤，可靠性更好。如果是边远山区不方便铺光纤，5G可能是好的替代。有总比没有好。 远程挖掘机 —— 微博上看到的，挖掘机通过5G联网，驾驶员在机房操控，安全性更高。 远程办公、远程上课 —— 这些光纤都能做到。对于山村里不方便通光纤的，是好事。但是山村里的困难是超乎我们想象的。看新闻，因为疫情不得不在家上课，几个小孩子共用一个手机，流量又贵，不舍得用。太难了。 AR/VR —— 不看好。光纤，百兆宽带都这么普及了，AR/VR发展不好的根本原因应该是AR/VR本身吧，5G救不了ARVR。 无人驾驶 —— 不看好。移动通信中的延时一直保持很低很困难吧，比如短时间太多人/设备使用基站导致拥挤。可靠性太重要了，毕竟汽车是高速移动的，稍微的疏忽就是惨痛代价。  会不会带来下一个“移动互联网” 之所以大家对5G这么关注，就是在想，5G能否像3/4G一样带动下一个移动互联网（电商、O2O、移动支付、短视频、直播）。
先来看看移动互联网是怎么发展起来的。
 苹果公司发布iPhone，智能手机的火种。 小米等一众国产品牌的低价策略，带来了智能手机的普及。 国务院要求运营商资费下降。大家都用得起流量。 总结就是：智能手机普及 + 运营商资费下降。 其中智能手机，这是从无到有，从0到1的奇迹，颠覆了之前的所有手机。（好久没有让人激动的科技产品了，苹果的创新能力也是在备受质疑，因为大家期望太高了。） 其中运营商资费下降，这是国家权力。（知乎用户：钱都让互联网企业赚走了，通讯行业等同于修路。）  5G目前的作用仅限于加快了与基站之间的通信速度，假如说3G完成了从0到1，4G完成了从1到10，那么5G是从10到100吧。根据边际效应准则，后面的收益会越来越少。
很难看出能带来什么样的产业革命来。
5G的利益关系 利好：
 5G基站厂商。华为为主。 国家层面。5G是国家战略，是政治正确。其一，靠专利技术薅世界羊毛，从国家角度是有利可图的。其二，带动产业发展，拉动GDP。 手机终端厂商。近年来手机上的创新的吸睛能力真不行，消费欲望降低。靠5G这个卖点，能赚一波眼球。 还有众多蹭5G话题流量的：广告公司、创业公司。  利坏：
 三大运营商。基站投入是高成本的，为了发展5G生态，国家也会限制套餐费用，影响其收入。还好是国企。  参考：https://www.zhihu.com/question/342366514/answer/820922564
杂言 真正有颠覆性的，真正能突破人们想象力的，都在基础科学。比如材料科学，电池技术现在是瓶颈。离我很远。我做的事情真的对人类价值不大，只是个普通人，只能做到赚钱养家对家庭做贡献了。</description>
    </item>
    
    <item>
      <title>Implicit：推荐系统协同过滤库的测评</title>
      <link>https://xujiahua.github.io/posts/20200211-recsys-implicit/</link>
      <pubDate>Tue, 11 Feb 2020 14:34:27 +0800</pubDate>
      
      <guid>https://xujiahua.github.io/posts/20200211-recsys-implicit/</guid>
      <description>简介 Implicit是一个推荐系统协同过滤库。所谓协同过滤，只用到了user、item的ID和user、item交互后的评分（或是某个度量）。
GitHub：https://github.com/benfred/implicit
安装 pip install implicit 其他选择 自己照着「推荐系统实战」里的基于物品推荐的相似度公式（类似关联分析中的Lift公式），也写了一个item-item recommender。
 看推荐结果，自己写与implicit ALS有点差别，与implicit Cosine的结果差不多。 纯python，没有并行，没有用C/C++，果然很慢。 自己写一个的目的，除了练手，也想看看implicit的效果具体怎么样。 生产使用，不建议用自己手写的，水平真的有限。  Spark Mllib怎么样。
 Spark Mllib支持分布式，感觉没必要。本来Implicit单机就很强了。只要把数据导出来，导到训练机器上即可。 Spark分布式，扩展性好，但是性能并没有Implicit那么好。 Spark还得搭配HDFS，真的蛮重的，开发效率也没那么好。  背后的原理 今早根据作者文章Distance Metrics for Fun and Profit，整理了一份读后感，把之前的知识给梳理起来了。这篇文章主要介绍的是距离公式，用于K近邻的推荐算法。
协同过滤算法分两类：
 基于记忆的，K近邻算法（基于“距离”公式），建议使用BM25。优势在读后感中有写。（实际效果怎么样，还得看数据） 基于模型的，矩阵分解算法，建议使用 (implicit) ALS，其变种是支持implicit dataset的。矩阵分解算法SGD，只适用于评分。  效果，见测评图。ALS的效果是最好的了。这是在movielens100k的结果。
看起来指标都很低，「推荐系统实战」里差不多这个结果。
遇到的小问题  K近邻算法的model没有model.recommend_all方法。 model.recommend与model.recommend_all的返回数据结构不同，model.recommend是包含评分的，而model.recommend_all只返回ID。 recommend(0)，传入第一个用户ID，与recommend_all[0]，取出第一个用户ID，两者的结果是不一样的，不管用什么算法都一样！去年12月就有这样的issue：https://github.com/benfred/implicit/issues/299，我解决了，回复了这个issue。  猜想，因为recommend方法用到indices的原因？试着重新构建下user-item维度的sparse matrix。改完后，果然就一致了。技术原因，sparse matrix transpose后，indices是未转置前的indices，没有变化。 具体地，user_items.indices与item_users.T.indices是不同的。
ALS算法在Mac上表现稀烂啊，同样的代码。ALS_Faiss还报错了。先在Linux上用吧。  Mac上的ALS效果：
Linux上的ALS效果：
冷启动 一开始并没有那么多行为数据。训练出的结果肯定是挺一般的。这时候用标签推荐，运营根据经验配置标签和推荐的映射即可。
考虑用户特征、商品特征（而不仅仅是ID） 如果有user特征、item特征，可以使用更一般的监督学习算法。或是使用factorization machine。</description>
    </item>
    
  </channel>
</rss>