<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>许嘉华的博客</title>
    <link>https://xujiahua.github.io/</link>
    <description>Recent content on 许嘉华的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 21 Apr 2020 09:01:38 +0800</lastBuildDate>
    
        <atom:link href="https://xujiahua.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    
        <item>
        <title>consul 小结</title>
        <link>https://xujiahua.github.io/posts/20200421-use-consul/</link>
        <pubDate>Tue, 21 Apr 2020 09:01:38 +0800</pubDate>
        
        <guid>https://xujiahua.github.io/posts/20200421-use-consul/</guid>
        <description>许嘉华的博客 https://xujiahua.github.io/posts/20200421-use-consul/ -&lt;h2 id=&#34;简单介绍&#34;&gt;简单介绍&lt;/h2&gt;
&lt;p&gt;hashicorp 对 Consul 的定位是服务间网络方案。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Consul is a service networking solution to connect and secure services across any runtime platform and public or private cloud. &lt;a href=&#34;https://www.consul.io/&#34;&gt;https://www.consul.io/&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;官方的两个 use case 就是 service discovery, service mesh。&lt;/p&gt;
&lt;h3 id=&#34;service-discovery&#34;&gt;service discovery&lt;/h3&gt;
&lt;p&gt;与 etcd 比起来，Consul 的服务发现是开箱即用的。优点如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First class 的服务注册和获取接口。不需要像 etcd 那样在 kv 存储基础上做包装。&lt;/li&gt;
&lt;li&gt;服务注册，可以是consul 命令，也可以是HTTP API。&lt;/li&gt;
&lt;li&gt;获取服务注册表，除了 HTTP 接口，还可以使用  DNS 查询接口。&lt;/li&gt;
&lt;li&gt;健康检查。服务注册的时候可以提供健康检查项。健康检查机制保证了拿到的服务注册表是“健康”的。健康检查也包括节点的检查。单纯利用 consul 健康检查这个功能，consul 就是一个分布式监控工具。&lt;/li&gt;
&lt;li&gt;Web 管理界面。节点、服务、健康与否一目了然。&lt;/li&gt;
&lt;li&gt;Watch 功能。通过 blocking queries/ long-polling HTTP API 的方式得到服务注册表的改变的通知。&lt;/li&gt;
&lt;li&gt;跨数据中心（取资源）。When a request is made for a resource in another datacenter, the local Consul servers forward an RPC request to the remote Consul servers for that resource and return the results. &lt;a href=&#34;https://learn.hashicorp.com/consul/security-networking/datacenters#data-replication&#34;&gt;https://learn.hashicorp.com/consul/security-networking/datacenters#data-replication&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;service-mesh&#34;&gt;service mesh&lt;/h3&gt;
&lt;p&gt;service discovery 只是微服务治理的初级阶段。作为服务请求方，通过 consul/ etcd 获取到服务注册表，下一步就是选择其中一个服务实例，发送请求。这个步骤叫做负载均衡。可以想象，客户端的代码会越来越重了。&lt;/p&gt;
&lt;p&gt;service mesh可以理解为 service discovery的升级版。为每个服务实例引入 sidecar proxy，接管服务实例的入、出流量。将 service discovery，load balancer 从 service 本身抽离出来，下沉到基础设施，也就是 sidecar proxy。&lt;/p&gt;
&lt;p&gt;sidecar proxy 上的功能还有更多扩展，比如：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;蓝绿部署，A/B 测试。打个比方，一个服务存在两个版本v1, v2，给v1分配80%的流量，给v2分配20%的流量。&lt;/li&gt;
&lt;li&gt;流量加密。authentication and authorization。&lt;/li&gt;
&lt;li&gt;服务指标的收集。比如HTTP协议的返回状态码。&lt;/li&gt;
&lt;li&gt;调用链追踪。&lt;/li&gt;
&lt;li&gt;Intention。可以设置服务与服务之间是否允许连接。Intentions define service based access control for services in the Consul service mesh and are used to control which services are allowed or not allowed to establish connections.&lt;/li&gt;
&lt;li&gt;这一切服务本身是无感知的。这也简化了应用的开发。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这就是 Consul Connect 的功能。与之对应的竞品是 Istio。&lt;/p&gt;
&lt;p&gt;当然，一切都有代价。&lt;strong&gt;给每个服务实例创建一个sidecar proxy，这在部署上需要做好准备。使用 Kubernetes 可以提高效率，会帮助自动创建 sidecar proxy。&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;key-value-store&#34;&gt;key value store&lt;/h3&gt;
&lt;p&gt;consul 也能当 kv store 使用，这是服务发现的底子。使用方式跟 etcd 差不多。&lt;/p&gt;
&lt;p&gt;etcd 官方与 consul 做了比较。consul 在存储扩展性上不够好。也缺少KEY多版本存储，不方便追溯历史。key value store 这个场景，etcd 是更合适的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/coreos/dbtester/tree/master/test-results/2018Q1-02-etcd-zookeeper-consul&#34;&gt;As it stands in Consul 1.0&lt;/a&gt;, the storage system does not scale as well as other systems like etcd or Zookeeper in key-value operations; systems requiring millions of keys will suffer from high latencies and memory pressure. The key value API is missing, most notably, multi-version keys, conditional transactions, and reliable streaming watches.&lt;/p&gt;
&lt;p&gt;etcd and Consul solve different problems. If looking for a distributed consistent key value store, etcd is a better choice over Consul. If looking for end-to-end cluster service discovery, etcd will not have enough features; choose Kubernetes, Consul, or SmartStack.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/etcd-io/etcd/blob/master/Documentation/learning/why.md#consul&#34;&gt;https://github.com/etcd-io/etcd/blob/master/Documentation/learning/why.md#consul&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;consul client agent的主要工作是健康检查，所以如果只是key value store的使用场景，可以直接与 consul server agent 交互，就像使用 etcd 那样。&lt;/p&gt;
&lt;h3 id=&#34;distributed-system-coordinate&#34;&gt;distributed system coordinate&lt;/h3&gt;
&lt;p&gt;分布式锁，分布式系统的协调 &lt;a href=&#34;https://www.consul.io/docs/internals/sessions.html&#34;&gt;https://www.consul.io/docs/internals/sessions.html&lt;/a&gt; 。使用场景比如Hadoop系统的选主。不过大部分开源分布式系统基本上使用zookeeper和etcd。&lt;/p&gt;
&lt;h2 id=&#34;核心概念&#34;&gt;核心概念&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;../../images/consul-arch-420ce04a.png&#34; alt=&#34;Consul Architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;Consul 架构图&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;名称&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;node&lt;/td&gt;
&lt;td&gt;每个 node 安装并运行 consul agent。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;consul server agent&lt;/td&gt;
&lt;td&gt;1）维护核心状态并基于共识算法 consensus protocol raft 参与leader选举。2）server节点一般建议3个或是5个。写压力大的集群，考虑升级服务器实例的配置和低延迟的存储。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;consul client agent&lt;/td&gt;
&lt;td&gt;1）当前节点、当前节点上的服务的健康检查。2）RPC请求转发到 consul server agent。3）每个主机都有一个agent的好处是，只要与本地agent通信。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Datacenter&lt;/td&gt;
&lt;td&gt;可以理解为一个 consul 集群。一个consul 集群至少有一个 consul server agent。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;LAN gossip pool&lt;/td&gt;
&lt;td&gt;单个 Datacenter 内，由 consul server agent 和 consul client agent 组成。pool内成员通过 gossip protocol 通信。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WAN gossip pool&lt;/td&gt;
&lt;td&gt;跨 Datacenter，由所有 Datacenter 内的 consul server agent 组成。可以跨网络通信。pool内成员通过 gossip protocol 通信。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;service&lt;/td&gt;
&lt;td&gt;一个service对应多个service实例，注册时使用相同的service_name，并使用不同的service_id区分实例。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;参考&#34;&gt;参考&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.consul.io/docs/internals/architecture.html&#34;&gt;https://www.consul.io/docs/internals/architecture.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.consul.io/docs/glossary.html&#34;&gt;https://www.consul.io/docs/glossary.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;consul-connect-体验&#34;&gt;Consul Connect 体验&lt;/h2&gt;
&lt;h3 id=&#34;1-quickstart-consul-connect&#34;&gt;1. Quickstart: Consul Connect&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../images/consul_connect_demo_service_flow.png&#34; alt=&#34;Flow diagram showing end user traffic being sent to the Dashboard Service at port 9002. The dashboard service makes requests for the counting service to the local Connect Proxy at port 5000. This traffic then traverses the Connect mesh over dynamic ports. The traffic exits the Connect mesh from the counting service&amp;rsquo;s local proxy. The proxy sends this traffic to the counting service itself at port 9003.&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Secure Service-to-Service Communication &lt;a href=&#34;https://learn.hashicorp.com/consul/developer-mesh/connect-services&#34;&gt;https://learn.hashicorp.com/consul/developer-mesh/connect-services&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;code &lt;a href=&#34;https://github.com/hashicorp/demo-consul-101&#34;&gt;https://github.com/hashicorp/demo-consul-101&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;简单说明：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;consul agent -dev -config-dir=&amp;quot;./demo-config-localhost&amp;quot; -node=laptop&lt;/code&gt; 这里已经完成了service和proxy的注册。这个可以后续脚本化，按需服务注册。&lt;/li&gt;
&lt;li&gt;service本身都不做服务发现和负载均衡。这些事情交给了 sidecar proxy。以dashboard service为例，&lt;code&gt;countingServiceURL = getEnvOrDefault(&amp;quot;COUNTING_SERVICE_URL&amp;quot;, &amp;quot;http://localhost:9001&amp;quot;)&lt;/code&gt;，默认读的是本地的9001端口，也就是sidecar proxy的绑定端口。这就是劫持流量。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;consul connect proxy -sidecar-for counting-1&lt;/code&gt; 需要手动为service创建proxy。这个可以后续脚本化。&lt;/li&gt;
&lt;li&gt;能看出Consul Connect /Service Mesh的好处了：不再侵入业务代码。重复的事情已下沉到基础设施，sidecar proxy处理。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;2-use-envoy&#34;&gt;2. use Envoy&lt;/h3&gt;
&lt;p&gt;Use Envoy with Connect &lt;a href=&#34;https://learn.hashicorp.com/consul/developer-mesh/connect-envoy&#34;&gt;https://learn.hashicorp.com/consul/developer-mesh/connect-envoy&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;3-canary-deployments-using-traffic-splitting-and-resolution&#34;&gt;3. Canary deployments using traffic splitting and resolution&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;../../images/consul-splitting-architecture.png&#34; alt=&#34;Architecture diagram of the splitting demo. A web service directly connects to two different versions of the API service through proxies. Consul configures those proxies.&#34;&gt;&lt;/p&gt;
&lt;p&gt;Traffic Splitting for Service Deployments &lt;a href=&#34;https://learn.hashicorp.com/consul/developer-mesh/consul-splitting&#34;&gt;https://learn.hashicorp.com/consul/developer-mesh/consul-splitting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;code &lt;a href=&#34;https://github.com/hashicorp/consul-demo-traffic-splitting&#34;&gt;https://github.com/hashicorp/consul-demo-traffic-splitting&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;4-zipkin-tracing&#34;&gt;4. Zipkin tracing&lt;/h3&gt;
&lt;p&gt;code &lt;a href=&#34;https://github.com/hashicorp/consul-demo-tracing/tree/master/jaeger&#34;&gt;https://github.com/hashicorp/consul-demo-tracing/tree/master/jaeger&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;基于-consul-微服务改造&#34;&gt;基于 Consul 微服务改造&lt;/h2&gt;
&lt;p&gt;不用 Kubernetes 和 Docker。&lt;/p&gt;
&lt;h3 id=&#34;服务间通信&#34;&gt;服务间通信&lt;/h3&gt;
&lt;p&gt;目的：应用对 consul 无感知，降低应用开发的复杂度。&lt;/p&gt;
&lt;p&gt;需要做如下准备：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;独立服务器安装 consul server agent 集群。负责维护集群状态。&lt;/li&gt;
&lt;li&gt;每台应用服务器都有 consul client agent 运行，并已经连入 consul server agent。负责健康检查，与请求转发。&lt;/li&gt;
&lt;li&gt;每台应用服务器上安装有 envoy 二进制文件。负责sidecar proxy的创建。&lt;/li&gt;
&lt;li&gt;服务注册、注销的工作交给应用的伙伴脚本。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;伙伴脚本的工作：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;启动应用。&lt;/li&gt;
&lt;li&gt;服务注册和健康检查。所需的信息，比如IP可以通过命令取、端口可以从应用启动信息取、服务名是固定的。&lt;/li&gt;
&lt;li&gt;启动 sidecar proxy 进程。&lt;/li&gt;
&lt;li&gt;伙伴脚本还需要监测应用的状态，应用不存在后，发起服务注销。&lt;/li&gt;
&lt;li&gt;consul的健康检查机制会自动把不健康的服务过滤掉，对伙伴脚本的要求没那么高了。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;对外服务&#34;&gt;对外服务&lt;/h3&gt;
&lt;p&gt;目的：动态更新 nginx upstream。&lt;/p&gt;
&lt;p&gt;目前，我们使用nginx作为对外服务。使用 consul-template，动态生成 nginx 配置（upstreams）并 reload nginx。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/consul-nginx-template-arch.png&#34; alt=&#34;NGINX and Consul template architecture&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;参考-1&#34;&gt;参考&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Load Balancing with NGINX and Consul Template &lt;a href=&#34;https://learn.hashicorp.com/consul/integrations/nginx-consul-template&#34;&gt;https://learn.hashicorp.com/consul/integrations/nginx-consul-template&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;consul-template &lt;a href=&#34;https://learn.hashicorp.com/consul/developer-configuration/consul-template&#34;&gt;https://learn.hashicorp.com/consul/developer-configuration/consul-template&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Manage local application configuration files using templates and data from etcd or consul 与consul-template的思路是一样的 &lt;a href=&#34;https://github.com/kelseyhightower/confd&#34;&gt;https://github.com/kelseyhightower/conf&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;一些资料&#34;&gt;一些资料&lt;/h2&gt;
&lt;p&gt;可能有帮助。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Getting Started &lt;a href=&#34;https://learn.hashicorp.com/consul?track=getting-started#getting-started&#34;&gt;https://learn.hashicorp.com/consul?track=getting-started#getting-started&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;基于consul构建golang系统分布式服务发现机制（使用Consul HTTP API） &lt;a href=&#34;https://segmentfault.com/a/1190000008471221&#34;&gt;https://segmentfault.com/a/1190000008471221&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Service registry bridge for Docker 监听Docker的Unix套接字来获取Docker容器启动和消亡时的事件，并且它会通过在事先配置好的一个可插拔的后端服务中创建新记录的形式自动完成容器的服务注册 &lt;a href=&#34;https://gliderlabs.github.io/registrator/latest/&#34;&gt;https://gliderlabs.github.io/registrator/latest/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
- https://xujiahua.github.io/posts/20200421-use-consul/ - </description>
        </item>
    
    
    
        <item>
        <title>etcd 小结</title>
        <link>https://xujiahua.github.io/posts/20200420-use_etcd/</link>
        <pubDate>Mon, 20 Apr 2020 14:12:38 +0800</pubDate>
        
        <guid>https://xujiahua.github.io/posts/20200420-use_etcd/</guid>
        <description>许嘉华的博客 https://xujiahua.github.io/posts/20200420-use_etcd/ -&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;
&lt;p&gt;取名有意思。Linux下的/etc目录放的是配置文件。etcd，etc代表配置，d代表distributed，代表分布式配置。&lt;/p&gt;
&lt;p&gt;特点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;designed to reliably store infrequently updated data and provide reliable watch queries &lt;a href=&#34;https://etcd.io/docs/v3.4.0/learning/data_model/&#34;&gt;https://etcd.io/docs/v3.4.0/learning/data_model/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;KV 核心用户接口&lt;/li&gt;
&lt;li&gt;MVCC Multi-version Concurrency Control 也确实能读历史版本&lt;/li&gt;
&lt;li&gt;Raft consensus algorithms 共识算法&lt;/li&gt;
&lt;li&gt;Watch 配置更新能及时&amp;quot;通知&amp;quot;应用&lt;/li&gt;
&lt;li&gt;RBAC 用户、角色、权限&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;基于 etcd 可以做哪些事情：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;配置中心。元数据存储。应用的配置集中存储在配置中心。&lt;/li&gt;
&lt;li&gt;服务发现。配置中心的一个特例。相比起来，consul的服务发现是开箱即用的。&lt;/li&gt;
&lt;li&gt;分布式锁。分布式系统协调。选主。像是Hadoop使用Zookeeper做Namenode的选主。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;vs. Consul。Consul 官方（https://www.consul.io/）定义的usecase是 service discovery和 service mesh。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;etcd and Consul solve different problems. If looking for a distributed consistent key value store, etcd is a better choice over Consul. If looking for end-to-end cluster service discovery, etcd will not have enough features; choose Kubernetes, Consul, or SmartStack. &lt;a href=&#34;https://github.com/etcd-io/etcd/blob/master/Documentation/learning/why.md#consul&#34;&gt;https://github.com/etcd-io/etcd/blob/master/Documentation/learning/why.md#consul&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果是分布式配置中心，etcd是更好的选择。&lt;/p&gt;
&lt;h3 id=&#34;参考&#34;&gt;参考&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;etcd versus other key-value stores &lt;a href=&#34;https://github.com/etcd-io/etcd/blob/master/Documentation/learning/why.md&#34;&gt;https://github.com/etcd-io/etcd/blob/master/Documentation/learning/why.md&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;etcd集群安装&#34;&gt;etcd集群安装&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/XUJiahua/linux_scripts/tree/master/etcd&#34;&gt;https://github.com/XUJiahua/linux_scripts/tree/master/etcd&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;常用操作&#34;&gt;常用操作&lt;/h2&gt;
&lt;p&gt;etcdctl的命令主要是kv操作，以及集群管理，和RBAC用户权限管理。&lt;/p&gt;
&lt;p&gt;参考 &lt;a href=&#34;https://etcd.io/docs/v3.4.0/demo/&#34;&gt;https://etcd.io/docs/v3.4.0/demo/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;kv相关&#34;&gt;KV相关&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;put foo &amp;quot;Hello World&amp;quot;&lt;/code&gt;  KV写&lt;/li&gt;
&lt;li&gt;&lt;code&gt;get foo&lt;/code&gt; KV读，还可以读历史版本的值&lt;/li&gt;
&lt;li&gt;&lt;code&gt;get web --prefix&lt;/code&gt; KV前缀读，适用于服务发现&lt;/li&gt;
&lt;li&gt;&lt;code&gt;del key&lt;/code&gt; KV删除&lt;/li&gt;
&lt;li&gt;&lt;code&gt;del k --prefix&lt;/code&gt; KV前缀删除&lt;/li&gt;
&lt;li&gt;&lt;code&gt;txn --interactive&lt;/code&gt; KV事务&lt;/li&gt;
&lt;li&gt;&lt;code&gt;watch stock1&lt;/code&gt; KV关注value变化，适用于服务发现，服务发生变动&lt;/li&gt;
&lt;li&gt;&lt;code&gt;watch stock --prefix&lt;/code&gt;  watch也支持前缀匹配&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;lease&#34;&gt;Lease&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;lease grant 300&lt;/code&gt; 创建300s的lease&lt;/li&gt;
&lt;li&gt;&lt;code&gt;put sample value --lease=2be7547fbc6a5afa&lt;/code&gt; 有效期300s的key。不设置 lease的put操作，值是永久存储的。&lt;/li&gt;
&lt;li&gt;keep-alive 刷新TTL, list, revoke。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;分布式锁&#34;&gt;分布式锁&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;lock mutex1&lt;/code&gt; 获取锁后，其他请求被block&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;集群管理&#34;&gt;集群管理&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;member list&lt;/code&gt; list etcd member，member命令还可以增加新node，node也包含不投票的。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;endpoint status&lt;/code&gt; 集群状态，能看到leader server&lt;/li&gt;
&lt;li&gt;&lt;code&gt;endpoint health&lt;/code&gt; 健康检查&lt;/li&gt;
&lt;li&gt;&lt;code&gt;snapshot save my.db&lt;/code&gt; 数据快照存储到本地&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;用户权限role-based-access-control&#34;&gt;用户权限（role based access control）&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;role add root&lt;/code&gt; 创建role&lt;/li&gt;
&lt;li&gt;&lt;code&gt;role grant-permission root readwrite foo&lt;/code&gt; role上赋予权限，这里赋予foo的读写权限&lt;/li&gt;
&lt;li&gt;&lt;code&gt;user add root&lt;/code&gt; 创建用户，并指定密码&lt;/li&gt;
&lt;li&gt;&lt;code&gt;user grant-role root root&lt;/code&gt; 赋予用户role&lt;/li&gt;
&lt;li&gt;&lt;code&gt;auth enable&lt;/code&gt; 开启认证，disable 即关闭auth。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--user=root:root put foo bar&lt;/code&gt; 操作时指定用户密码&lt;/li&gt;
&lt;/ol&gt;
- https://xujiahua.github.io/posts/20200420-use_etcd/ - </description>
        </item>
    
    
    
        <item>
        <title>k8s configmap 与热更新</title>
        <link>https://xujiahua.github.io/posts/20200417-kubernetes-configmap/</link>
        <pubDate>Fri, 17 Apr 2020 16:09:28 +0800</pubDate>
        
        <guid>https://xujiahua.github.io/posts/20200417-kubernetes-configmap/</guid>
        <description>许嘉华的博客 https://xujiahua.github.io/posts/20200417-kubernetes-configmap/ -&lt;h2 id=&#34;configmap-简介&#34;&gt;configmap 简介&lt;/h2&gt;
&lt;p&gt;官方介绍：使用 ConfigMap 配置 Pod  &lt;a href=&#34;https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-pod-configmap/&#34;&gt;https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-pod-configmap/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;他人总结：ConfigMap &lt;a href=&#34;https://jimmysong.io/kubernetes-handbook/concepts/configmap.html&#34;&gt;https://jimmysong.io/kubernetes-handbook/concepts/configmap.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;稍微总结下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每个configmap都有一个名字，名字全局唯一（命名空间内），重复创建会报错。&lt;/li&gt;
&lt;li&gt;每个configmap本身是键值对。&lt;/li&gt;
&lt;li&gt;configmap可以通过环境变量的方式让Pod内容器读取。&lt;/li&gt;
&lt;li&gt;configmap可以通过挂载文件的方式让Pod内容器读取。k8s每隔一段时间同步configmap，如果有更新的话。当然，应用本身是不知道的。这个定时更新感觉有点鸡肋。&lt;/li&gt;
&lt;li&gt;configmap更新，不会自动重启应用。只能人工方式，滚动重启应用。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;把配置更新也当作一次应用变更看待，心情就好很多了。&lt;/p&gt;
&lt;p&gt;官方不支持热更新，所以有了各种技巧，提高效率。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;create a new ConfigMap with the changes you want to make, and point your deployment at the new ConfigMap &lt;a href=&#34;https://stackoverflow.com/a/40624029/820682&#34;&gt;https://stackoverflow.com/a/40624029/820682&lt;/a&gt; 因为 deployment 文件变化了，触发滚动重启。&lt;/li&gt;
&lt;li&gt;还有deployment 文件中配置 configmap hash值的。配置变化，hash值变化，deployment变化，滚动重启，一级级联动。 &lt;a href=&#34;https://blog.questionable.services/article/kubernetes-deployments-configmap-change/&#34;&gt;https://blog.questionable.services/article/kubernetes-deployments-configmap-change/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;还有使用sidecar的方式做热更新的，太复杂了 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/57570231&#34;&gt;https://zhuanlan.zhihu.com/p/57570231&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;关于热更新&#34;&gt;关于热更新&lt;/h2&gt;
&lt;p&gt;configmap的更新，容器化应用是无感知的。configmap这种方式没有推送更新到应用内的机制，要实现热更新过于复杂。&lt;/p&gt;
&lt;p&gt;k8s最核心的功能还是自动部署、伸缩、容器管理以及资源分配。微服务架构还是得需要其他框架来辅助的。&lt;/p&gt;
&lt;p&gt;配置热更新应用，就选择 etcd, consul 吧，有 watch 功能。&lt;/p&gt;
- https://xujiahua.github.io/posts/20200417-kubernetes-configmap/ - </description>
        </item>
    
    
    
        <item>
        <title>etcd api client 请求重试逻辑</title>
        <link>https://xujiahua.github.io/posts/20200417-etcd_client/</link>
        <pubDate>Fri, 17 Apr 2020 15:05:51 +0800</pubDate>
        
        <guid>https://xujiahua.github.io/posts/20200417-etcd_client/</guid>
        <description>许嘉华的博客 https://xujiahua.github.io/posts/20200417-etcd_client/ -&lt;p&gt;使用 Consul 作为配置中心，按照官方的说法，没必要创建 &lt;code&gt;consul client&lt;/code&gt; 节点。那么直接连 &lt;code&gt;consul server&lt;/code&gt; 就好了。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Running an agent is not required for discovering other services or getting/setting key/value data. The agent is responsible for health checking the services on the node as well as the node itself.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.consul.io/intro/index.html#basic-architecture-of-consul&#34;&gt;https://www.consul.io/intro/index.html#basic-architecture-of-consul&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Consul api client (&lt;a href=&#34;https://github.com/hashicorp/consul/tree/master/api&#34;&gt;https://github.com/hashicorp/consul/tree/master/api&lt;/a&gt;) 目前只能接收一个server地址。那么这个server地址得保证高可用才行啊。&lt;/p&gt;
&lt;p&gt;etcd api client (&lt;a href=&#34;https://github.com/etcd-io/etcd/tree/master/client&#34;&gt;https://github.com/etcd-io/etcd/tree/master/client&lt;/a&gt;) 倒是能接收多个server地址，看看 etcd 是怎么做的。&lt;/p&gt;
&lt;h3 id=&#34;etcd-api-client&#34;&gt;etcd api client&lt;/h3&gt;
&lt;p&gt;创建了 httpClusterClient。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200417151856806.png&#34; alt=&#34;image-20200417151856806&#34;&gt;&lt;/p&gt;
&lt;p&gt;多个 endpoint 的处理核心逻辑。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200417152639174.png&#34; alt=&#34;image-20200417152639174&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;pinned 用于记录好用的连接地址的index，优先使用这个地址。&lt;/li&gt;
&lt;li&gt;context 类错误，比如取消请求，直接退出。&lt;/li&gt;
&lt;li&gt;遇到 5xx 类错误，服务端错误。需要考虑是否重试了。&lt;/li&gt;
&lt;li&gt;isOneShot 标记，true 代表是 Set/Delete 操作，请求失败不再重试。应该跟请求是否幂等有关。&lt;/li&gt;
&lt;li&gt;可以重试的请求，重试直到成功或是循环结束。&lt;/li&gt;
&lt;/ol&gt;
- https://xujiahua.github.io/posts/20200417-etcd_client/ - </description>
        </item>
    
    
    
        <item>
        <title>机器学习在线推理部署方案：Cortex</title>
        <link>https://xujiahua.github.io/posts/20200416-cortex/</link>
        <pubDate>Thu, 16 Apr 2020 11:33:18 +0800</pubDate>
        
        <guid>https://xujiahua.github.io/posts/20200416-cortex/</guid>
        <description>许嘉华的博客 https://xujiahua.github.io/posts/20200416-cortex/ -&lt;h2 id=&#34;cortex-介绍&#34;&gt;Cortex 介绍&lt;/h2&gt;
&lt;p&gt;官方网站：Deploy machine learning models in production &lt;a href=&#34;https://www.cortex.dev/&#34;&gt;https://www.cortex.dev/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub &lt;a href=&#34;https://github.com/cortexlabs/cortex&#34;&gt;https://github.com/cortexlabs/cortex&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The CLI sends configuration and code to the cluster every time you run &lt;code&gt;cortex deploy&lt;/code&gt;. Each model is loaded into a Docker container, along with any Python packages and request handling code. The model is exposed as a web service using Elastic Load Balancing (ELB), TensorFlow Serving, and ONNX Runtime. The containers are orchestrated on Elastic Kubernetes Service (EKS) while logs and metrics are streamed to CloudWatch.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;CLI工具将配置文件和代码推送到集群。模型连同Python依赖包和请求处理的代码被Docker容器打包。使用AWS ELB等暴露出Web服务。容器使用AWS EKS编排，日志和指标会推送到AWS CloudWatch。&lt;/p&gt;
&lt;h3 id=&#34;key-features&#34;&gt;Key Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multi framework:&lt;/strong&gt; Cortex supports TensorFlow, PyTorch, scikit-learn, XGBoost, and more. 支持主流的机器学习、深度学习框架&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Autoscaling:&lt;/strong&gt; Cortex automatically scales APIs to handle production workloads.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU / GPU support:&lt;/strong&gt; Cortex can run inference on CPU or GPU infrastructure. CPU/GPU都支持。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spot instances:&lt;/strong&gt; Cortex supports EC2 spot instances.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Rolling updates:&lt;/strong&gt; Cortex updates deployed APIs without any downtime. 依赖k8s的能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Log streaming:&lt;/strong&gt; Cortex streams logs from deployed models to your CLI. 依赖CloudWatch的能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Prediction monitoring:&lt;/strong&gt; Cortex monitors network metrics and tracks predictions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Minimal configuration:&lt;/strong&gt; Cortex deployments are defined in a single &lt;code&gt;cortex.yaml&lt;/code&gt; file.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;我的体验&#34;&gt;我的体验&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;与AWS深度绑定。对私有云、国内公有云不够友好了。&lt;/li&gt;
&lt;li&gt;依赖Kubernetes服务。&lt;/li&gt;
&lt;li&gt;依赖云存储服务，比如S3、OSS。cortex deploy的建议：cortex will zip files and upload them to the cluster; we recommend that you upload large files/directories (e.g. models) to s3 and download them in your api&amp;rsquo;s &lt;strong&gt;init&lt;/strong&gt; function,&lt;/li&gt;
&lt;li&gt;有一套Python API服务的模板，并使用Docker封装。比如 &lt;a href=&#34;https://github.com/cortexlabs/cortex/blob/master/images/python-serve/Dockerfile&#34;&gt;https://github.com/cortexlabs/cortex/blob/master/images/python-serve/Dockerfile&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;推理Predictor类按照Cortex定义的接口规范实现，所在目录挂载到 /mnt/project 目录。&lt;/li&gt;
&lt;li&gt;启动容器，预置脚本会安装&lt;code&gt;requirement.txt&lt;/code&gt;，并动态加载Predictor类。&lt;/li&gt;
&lt;li&gt;使用者只需要处理模型训练和按照规范定义Predictor类。重复的API定义、部署、扩容，都已经被隐藏掉了。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;代码结构&#34;&gt;代码结构&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;cli 目录：客户端。deploy操作，就是把本地的配置文件压缩成zip包上传。&lt;/li&gt;
&lt;li&gt;pkg/operator 目录：服务端。deploy接口在此：zip包会上传到S3，编程的方式申请k8s资源（deployment, service, virtualService），直接向k8s API server发送请求。&lt;/li&gt;
&lt;li&gt;pkg/lib 目录：比较核心的Go代码。&lt;/li&gt;
&lt;li&gt;pkg/workloads/cortex/downloader 目录：在k8s中作为InitContainer，用于下载配置文件到Pod中。&lt;/li&gt;
&lt;li&gt;pkg/workloads 目录：推理服务的代码，比较通用。pkg/workloads/cortex/serve/run.sh 留了个口子 &lt;code&gt;/mnt/project&lt;/code&gt;，每个服务个性化的部分留给开发者（遵从规范）。&lt;/li&gt;
&lt;li&gt;images 目录：镜像文件。包括 operator 的镜像文件，也包括推理服务的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;具体看看创建推理服务容器的过程。&lt;/p&gt;
&lt;p&gt;申请 K8s Deployment。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200416151230669.png&#34; alt=&#34;image-20200416151230669&#34;&gt;&lt;/p&gt;
&lt;p&gt;这里分为三大类。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200416151318515.png&#34; alt=&#34;image-20200416151318515&#34;&gt;&lt;/p&gt;
&lt;p&gt;关注PythonPredictorType。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200416151537448.png&#34; alt=&#34;image-20200416151537448&#34;&gt;&lt;/p&gt;
&lt;p&gt;传入 InitContainer （代码见 pkg/workloads/cortex/downloader）的参数如下，将S3上存储的配置信息和predictor文件等下载到Pod中的 &lt;code&gt;/mnt/project&lt;/code&gt;目录（&lt;code&gt;_emptyDirMountPath = &amp;quot;/mnt&amp;quot;&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200416152324873.png&#34; alt=&#34;image-20200416152324873&#34;&gt;&lt;/p&gt;
&lt;p&gt;因为Pod内容器共享存储空间，这样推理服务容器就能读取到 &lt;code&gt;/mnt/project&lt;/code&gt;了。&lt;/p&gt;
&lt;h2 id=&#34;cortex-镜像体验&#34;&gt;cortex 镜像体验&lt;/h2&gt;
&lt;p&gt;下载地址 &lt;a href=&#34;https://hub.docker.com/r/cortexlabs/python-serve/tags&#34;&gt;https://hub.docker.com/r/cortexlabs/python-serve/tags&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;容器内代码也是依赖S3的，不修改还不能直接使用🤣。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200416161622829.png&#34; alt=&#34;image-20200416161622829&#34;&gt;&lt;/p&gt;
&lt;p&gt;到此为止吧。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker run -e CORTEX_SERVING_PORT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5000&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        -e CORTEX_WORKERS_PER_REPLICA&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        -e CORTEX_MAX_WORKER_CONCURRENCY&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        -e CORTEX_THREADS_PER_WORKER&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        -e CORTEX_SO_MAX_CONN&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2048&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        -e CORTEX_CACHE_DIR&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/mnt/spec &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        -e CORTEX_VERSION&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0.15.1 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        -v &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;pwd&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;:/mnt/project &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        -v &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;pwd&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;:/mnt/spec &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;        -p 5000:5000 cortexlabs/python-serve:0.15.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;- https://xujiahua.github.io/posts/20200416-cortex/ - </description>
        </item>
    
    
    
        <item>
        <title>k8s 应用日志收集</title>
        <link>https://xujiahua.github.io/posts/20200414-k8s-logging/</link>
        <pubDate>Tue, 14 Apr 2020 09:53:35 +0800</pubDate>
        
        <guid>https://xujiahua.github.io/posts/20200414-k8s-logging/</guid>
        <description>许嘉华的博客 https://xujiahua.github.io/posts/20200414-k8s-logging/ -&lt;h2 id=&#34;k8s-日志收集架构&#34;&gt;k8s 日志收集架构&lt;/h2&gt;
&lt;p&gt;以下是比较一般、普适的架构。更多参考：Kubernetes 日志架构 &lt;a href=&#34;https://kubernetes.io/zh/docs/concepts/cluster-administration/logging/&#34;&gt;https://kubernetes.io/zh/docs/concepts/cluster-administration/logging/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/logging-with-node-agent.png&#34; alt=&#34;使用节点日志记录代理&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;容器化应用将日志写入&lt;code&gt;stdout&lt;/code&gt;、&lt;code&gt;stderr&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;Docker容器引擎将&lt;code&gt;stdout&lt;/code&gt;、&lt;code&gt;stderr&lt;/code&gt;流重定向到日志驱动，比如默认的json-file。&lt;/li&gt;
&lt;li&gt;json-file日志驱动将日志写入到（宿主机上的）文件。&lt;/li&gt;
&lt;li&gt;日志收集工具以DaemonSet的形式安装在每个节点。&lt;/li&gt;
&lt;li&gt;日志收集工具监听文件变化，并将日志写入到日志中心服务。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;k8s-日志收集细节&#34;&gt;k8s 日志收集细节&lt;/h2&gt;
&lt;h3 id=&#34;实战&#34;&gt;实战&lt;/h3&gt;
&lt;p&gt;可以直接参考以下教程：minikube创建了一个Kubernetes集群，Fluentd收集日志，存入ElasticSearch，使用Kibana查看日志。典型的EFK技术栈。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Logging in Kubernetes with Elasticsearch, Kibana, and Fluentd &lt;a href=&#34;https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/&#34;&gt;https://mherman.org/blog/logging-in-kubernetes-with-elasticsearch-Kibana-fluentd/&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在Kibana上看收集到的日志。能看到日志收集工具也采集了容器、镜像、Pod有关的信息。这些上下文信息能让人定位到是哪个应用在生产日志。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200414104511399.png&#34; alt=&#34;image-20200414104511399&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;fluentd-收集上下文信息&#34;&gt;fluentd 收集上下文信息&lt;/h3&gt;
&lt;p&gt;Docker &lt;code&gt;json-file&lt;/code&gt; 日志驱动写文件，并不记录上下文信息。 &lt;a href=&#34;https://docs.docker.com/config/containers/logging/json-file/&#34;&gt;https://docs.docker.com/config/containers/logging/json-file/&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{&amp;quot;log&amp;quot;:&amp;quot;Log line is here\n&amp;quot;,&amp;quot;stream&amp;quot;:&amp;quot;stdout&amp;quot;,&amp;quot;time&amp;quot;:&amp;quot;2019-01-01T11:11:11.111111111Z&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;上文中使用的日志收集镜像是 &lt;code&gt;fluent/fluentd-kubernetes-daemonset:v1.3-debian-elasticsearch&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;具体代码路径在此 &lt;a href=&#34;https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.3/debian-elasticsearch&#34;&gt;https://github.com/fluent/fluentd-kubernetes-daemonset/tree/master/docker-image/v1.3/debian-elasticsearch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;收集容器目录下的日志。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200414105706563.png&#34; alt=&#34;image-20200414105706563&#34;&gt;&lt;/p&gt;
&lt;p&gt;使用&lt;code&gt;kubernetes_metadata&lt;/code&gt;这个第三方插件获取容器相关的上下文信息。这里是通过请求API server得到metadata的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200414105752287.png&#34; alt=&#34;image-20200414105752287&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubernetes_metadata&lt;/code&gt; 插件地址 &lt;a href=&#34;https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter&#34;&gt;https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;插件中有缓存metadata的选项，不用担心每处理一条日志，就要向API server发送请求。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200414111028867.png&#34; alt=&#34;image-20200414111028867&#34;&gt;&lt;/p&gt;
- https://xujiahua.github.io/posts/20200414-k8s-logging/ - </description>
        </item>
    
    
    
        <item>
        <title>Metabase &#43; Spark SQL</title>
        <link>https://xujiahua.github.io/posts/20200410-metabase-spark-sql/</link>
        <pubDate>Fri, 10 Apr 2020 16:41:53 +0800</pubDate>
        
        <guid>https://xujiahua.github.io/posts/20200410-metabase-spark-sql/</guid>
        <description>许嘉华的博客 https://xujiahua.github.io/posts/20200410-metabase-spark-sql/ -&lt;p&gt;这是大数据 BI 平台的第二步，BI 工具的搭建。假设已经配置好 Spark SQL JDBC Server，并启用了Kerberos。参考  &lt;a href=&#34;https://xujiahua.github.io/posts/20200410-spark-thrift-server-cdh/&#34;&gt;https://xujiahua.github.io/posts/20200410-spark-thrift-server-cdh/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这里，我们选择了开源产品 Metabase。&lt;/p&gt;
&lt;p&gt;最终，大数据 BI 平台，是由 1) 以Metabase作为BI可视化，2) 由HDFS（分布式文件存储） + parquet（列式数据存储格式）+ Hive metastore（SQL表结构信息维护） + Spark SQL（批处理引擎）组合的OLAP数据库组成。&lt;/p&gt;
&lt;h2 id=&#34;metabase-简介&#34;&gt;Metabase 简介&lt;/h2&gt;
&lt;p&gt;Metabase is the easy, open source way for everyone in your company to ask questions and learn from data.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.metabase.com/&#34;&gt;https://www.metabase.com/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;数据库支持&#34;&gt;数据库支持&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;BigQuery&lt;/li&gt;
&lt;li&gt;Druid&lt;/li&gt;
&lt;li&gt;Google Analytics&lt;/li&gt;
&lt;li&gt;H2&lt;/li&gt;
&lt;li&gt;MongoDB&lt;/li&gt;
&lt;li&gt;MySQL/MariaDB&lt;/li&gt;
&lt;li&gt;PostgreSQL&lt;/li&gt;
&lt;li&gt;Presto&lt;/li&gt;
&lt;li&gt;Amazon Redshift&lt;/li&gt;
&lt;li&gt;Snowflake&lt;/li&gt;
&lt;li&gt;Spark SQL&lt;/li&gt;
&lt;li&gt;SQLite&lt;/li&gt;
&lt;li&gt;SQL Server&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&#34;https://www.metabase.com/docs/latest/faq/setup/which-databases-does-metabase-support.html&#34;&gt;https://www.metabase.com/docs/latest/faq/setup/which-databases-does-metabase-support.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这里有我们需要的Spark SQL，我们的大数据集群可以支持。比较遗憾的是没有Impala。&lt;/p&gt;
&lt;h2 id=&#34;metabase-安装&#34;&gt;Metabase 安装&lt;/h2&gt;
&lt;h3 id=&#34;mysql&#34;&gt;MySQL&lt;/h3&gt;
&lt;p&gt;使用MySQL作为元数据存储。复用之前CDH的MySQL实例。&lt;/p&gt;
&lt;p&gt;创建数据库、用户。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-SQL&#34; data-lang=&#34;SQL&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;-- 适合MySQL5.7及以上版本，支持更大的max key length。一个字符使用四个字节。
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;DATABASE&lt;/span&gt; metabase CHARACTER &lt;span style=&#34;color:#66d9ef&#34;&gt;SET&lt;/span&gt; utf8mb4 &lt;span style=&#34;color:#66d9ef&#34;&gt;COLLATE&lt;/span&gt; utf8mb4_unicode_ci;
&lt;span style=&#34;color:#75715e&#34;&gt;-- 适合MySQL5.6及以下版本。一个字符使用3个字节。
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;DATABASE&lt;/span&gt; metabase CHARACTER &lt;span style=&#34;color:#66d9ef&#34;&gt;SET&lt;/span&gt; utf8 &lt;span style=&#34;color:#66d9ef&#34;&gt;COLLATE&lt;/span&gt; utf8_unicode_ci;

&lt;span style=&#34;color:#75715e&#34;&gt;-- % 表示不限制host
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;USER&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;metabase&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;@&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;%&amp;#39;&lt;/span&gt; IDENTIFIED &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;metabase&amp;#39;&lt;/span&gt;;
&lt;span style=&#34;color:#66d9ef&#34;&gt;GRANT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;ALL&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;ON&lt;/span&gt; metabase.&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TO&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;metabase&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;@&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;%&amp;#39;&lt;/span&gt;;

FLUSH &lt;span style=&#34;color:#66d9ef&#34;&gt;PRIVILEGES&lt;/span&gt;;
&lt;span style=&#34;color:#75715e&#34;&gt;-- https://dev.mysql.com/doc/refman/5.7/en/grant.html
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;常见问题&#34;&gt;常见问题&lt;/h4&gt;
&lt;p&gt;mysql 5.7与5.6的区别。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;767 bytes is the &lt;a href=&#34;http://dev.mysql.com/doc/refman/5.1/en/create-index.html&#34;&gt;stated prefix limitation&lt;/a&gt; for InnoDB tables in MySQL version 5.6 (and prior versions). It&amp;rsquo;s 1,000 bytes long for MyISAM tables. In MySQL version 5.7 and upwards this limit has been increased to 3072 bytes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Caused by: java.sql.SQLException: Specified key was too long; max key length is 767 bytes&lt;/p&gt;
&lt;p&gt;Caused by: liquibase.exception.DatabaseException: (conn=175553) Specified key was too long; max key length is 767 bytes [Failed SQL: CREATE TABLE &lt;code&gt;metabase&lt;/code&gt;.&lt;code&gt;core_organization&lt;/code&gt; (&lt;code&gt;id&lt;/code&gt; INT AUTO_INCREMENT NOT NULL, &lt;code&gt;slug&lt;/code&gt; VARCHAR(254) NOT NULL, &lt;code&gt;name&lt;/code&gt; VARCHAR(254) NOT NULL, &lt;code&gt;description&lt;/code&gt; TEXT NULL, &lt;code&gt;logo_url&lt;/code&gt; VARCHAR(254) NULL, &lt;code&gt;inherits&lt;/code&gt; BIT(1) NOT NULL, CONSTRAINT &lt;code&gt;PK_CORE_ORGANIZATION&lt;/code&gt; PRIMARY KEY (&lt;code&gt;id&lt;/code&gt;), UNIQUE (&lt;code&gt;slug&lt;/code&gt;))]&lt;/p&gt;
&lt;h3 id=&#34;metabase&#34;&gt;Metabase&lt;/h3&gt;
&lt;p&gt;下载metabase。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 以 /opt/metabase 为工作目录&lt;/span&gt;
mkdir /opt/metabase &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; cd /opt/metabase
&lt;span style=&#34;color:#75715e&#34;&gt;# 下载最新版本的metabase，参考 https://www.metabase.com/start/jar.html&lt;/span&gt;
wget https://downloads.metabase.com/v0.35.1/metabase.jar
&lt;span style=&#34;color:#75715e&#34;&gt;# 创建插件目录&lt;/span&gt; 
mkdir plugins
&lt;span style=&#34;color:#75715e&#34;&gt;# Spark SQL的驱动，jar包内置的有问题&lt;/span&gt;
wget https://s3.amazonaws.com/sparksql-deps/metabase-sparksql-deps-1.2.1.spark2-standalone.jar -O plugins/metabase-sparksql-deps-1.2.1.spark2-standalone.jar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;启动脚本 start.sh。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#!/usr/bin/bash
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
export MB_DB_TYPE&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mysql
export MB_DB_DBNAME&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;metabase
export MB_DB_PORT&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3306&lt;/span&gt;
export MB_DB_USER&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;metabase
export MB_DB_PASS&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;metabase
export MB_DB_HOST&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;172.31.0.100

kinit -kt hive.xh-hd2-peggy-dost000003.keytab hive/xh-hd2-peggy-dost000003@PEGGY.LING

mkdir -p logs

nohup java -Djavax.security.auth.useSubjectCredsOnly&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;false -jar metabase.jar  &amp;gt;&amp;gt; logs/metabase.log 2&amp;gt;&amp;amp;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &amp;amp;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;停止脚本 stop.sh。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#!/usr/bin/bash
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
ps -ef | grep &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;metabase.jar&amp;#39;&lt;/span&gt; | grep -v &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;grep&amp;#39;&lt;/span&gt; | awk &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{print $2}&amp;#39;&lt;/span&gt; | xargs kill -9

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;文件夹结构如下。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;webapp@xh-hd2-peggy-dost000003 metabase&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;$ tree
├── hive.xh-hd2-peggy-dost000003.keytab
├── logs
│   └── metabase.log
├── metabase.jar
├── plugins
│   ├── metabase-sparksql-deps-1.2.1.spark2-standalone.jar
│   ├── sparksql.metabase-driver.jar
│   ├── ...
├── start.sh
└── stop.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;参考：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Metabase搭建手册：使用SparkSQL连接Hive &lt;a href=&#34;https://webcache.googleusercontent.com/search?q=cache:DcmHXWOc9woJ:https://immm.in/archives/24.html+&amp;amp;cd=5&amp;amp;hl=zh-CN&amp;amp;ct=clnk&amp;amp;gl=hk&#34;&gt;https://webcache.googleusercontent.com/search?q=cache:DcmHXWOc9woJ:https://immm.in/archives/24.html+&amp;amp;cd=5&amp;amp;hl=zh-CN&amp;amp;ct=clnk&amp;amp;gl=hk&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;配置-spark-sql-连接&#34;&gt;配置 Spark SQL 连接&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200410180255501.png&#34; alt=&#34;image-20200410180255501&#34;&gt;&lt;/p&gt;
&lt;p&gt;保存没有报错，就是成功了。&lt;/p&gt;
&lt;h4 id=&#34;常见问题-1&#34;&gt;常见问题&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;使用内置的驱动报错：Unrecognized Hadoop major version number: 3.1.1&lt;/li&gt;
&lt;li&gt;使用metabase-sparksql-deps-1.2.1.spark2-standalone.jar这个驱动报错，然而beeline连接是没问题的： transport.TSaslTransport :: SASL negotiation failure
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;第一个问题，暂时忽略，使用其他驱动。&lt;/p&gt;
&lt;p&gt;按理说，https://github.com/metabase/sparksql-deps 已经合并到 &lt;a href=&#34;https://github.com/metabase/metabase/tree/v0.35.1/modules/drivers/sparksql&#34;&gt;https://github.com/metabase/metabase/tree/v0.35.1/modules/drivers/sparksql&lt;/a&gt; 了。内置驱动就能用才对。&lt;/p&gt;
&lt;p&gt;能看到依赖hadoop-common包的版本差异。TODO: 还得细看。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200411071918161.png&#34; alt=&#34;image-20200411071918161&#34;&gt;&lt;/p&gt;
&lt;p&gt;依赖的hadoop版本是3.1.0。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200411072135265.png&#34; alt=&#34;image-20200411072135265&#34;&gt;&lt;/p&gt;
&lt;p&gt;内置Driver依赖3.1.1。报错信息也是包含3.1.1，是否切到3.1.0，重新打包就可以了？&lt;/p&gt;
&lt;p&gt;第二个问题需要设置Java参数 &lt;code&gt;-Djavax.security.auth.useSubjectCredsOnly=false&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;通过底层机制获取凭证信息，而不是通过应用执行认证操作。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200410173734177.png&#34; alt=&#34;image-20200410173734177&#34;&gt;&lt;/p&gt;
&lt;p&gt;参考：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How to connnect sparksql in Kerberos enviroment &lt;a href=&#34;https://discourse.metabase.com/t/how-to-connnect-sparksql-in-kerberos-enviroment/8290/2&#34;&gt;https://discourse.metabase.com/t/how-to-connnect-sparksql-in-kerberos-enviroment/8290/2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided https://stackoverflow.com/questions/32205087/javax-security-sasl-saslexception-gss-initiate-failed-caused-by-gssexception&lt;/li&gt;
&lt;li&gt;Below are listed some problems that may occur when attempting a login, and suggestions for solving them. &lt;a href=&#34;https://docs.oracle.com/javase/7/docs/technotes/guides/security/jgss/tutorials/Troubleshooting.html&#34;&gt;https://docs.oracle.com/javase/7/docs/technotes/guides/security/jgss/tutorials/Troubleshooting.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;与-superset-比较&#34;&gt;与 Superset 比较&lt;/h2&gt;
&lt;p&gt;Superset是另外一个开源的BI工具。但是使用过程中体验不佳：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;不支持多表JOIN。这个不能忍。&lt;/li&gt;
&lt;li&gt;直接写SQL，数据没法可视化。效率有点低下。&lt;/li&gt;
&lt;li&gt;外观丑陋。FlaskAppbuilder生成的前后端架子。&lt;/li&gt;
&lt;li&gt;Table源如果改了，建议删掉再添加，不然会有各种意外。&lt;/li&gt;
&lt;li&gt;交互体验差。自定义SELECT COUNT(cookie) as pv，死活搞不定。正确的使用方式是设置label，体验不直观。&lt;/li&gt;
&lt;li&gt;小bug多。用起来太容易烦躁了。对身心健康不好。&lt;/li&gt;
&lt;/ol&gt;
- https://xujiahua.github.io/posts/20200410-metabase-spark-sql/ - </description>
        </item>
    
    
    
        <item>
        <title>CDH6 启用 Spark Thrift Server</title>
        <link>https://xujiahua.github.io/posts/20200410-spark-thrift-server-cdh/</link>
        <pubDate>Fri, 10 Apr 2020 10:07:16 +0800</pubDate>
        
        <guid>https://xujiahua.github.io/posts/20200410-spark-thrift-server-cdh/</guid>
        <description>许嘉华的博客 https://xujiahua.github.io/posts/20200410-spark-thrift-server-cdh/ -&lt;p&gt;很遗憾，CDH版本的Spark阉割了Thrift Server。（可能与自家产品Impala有竞争关系的原因。）&lt;/p&gt;
&lt;p&gt;参考 &lt;a href=&#34;https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/spark.html#spark__d99299e107&#34;&gt;https://docs.cloudera.com/documentation/enterprise/6/6.3/topics/spark.html#spark__d99299e107&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ll /opt/cloudera/parcels/CDH/lib/spark/sbin/
total 84
-rwxr-xr-x 1 root root 2803 Nov  9 00:05 slaves.sh
-rwxr-xr-x 1 root root 1429 Nov  9 00:05 spark-config.sh
-rwxr-xr-x 1 root root 5689 Nov  9 00:05 spark-daemon.sh
-rwxr-xr-x 1 root root 1262 Nov  9 00:05 spark-daemons.sh
-rwxr-xr-x 1 root root 1190 Nov  9 00:05 start-all.sh
-rwxr-xr-x 1 root root 1274 Nov  9 00:05 start-history-server.sh
-rwxr-xr-x 1 root root 2050 Nov  9 00:05 start-master.sh
-rwxr-xr-x 1 root root 1877 Nov  9 00:05 start-mesos-dispatcher.sh
-rwxr-xr-x 1 root root 1423 Nov  9 00:05 start-mesos-shuffle-service.sh
-rwxr-xr-x 1 root root 1279 Nov  9 00:05 start-shuffle-service.sh
-rwxr-xr-x 1 root root 3151 Nov  9 00:05 start-slave.sh
-rwxr-xr-x 1 root root 1527 Nov  9 00:05 start-slaves.sh
-rwxr-xr-x 1 root root 1478 Nov  9 00:05 stop-all.sh
-rwxr-xr-x 1 root root 1056 Nov  9 00:05 stop-history-server.sh
-rwxr-xr-x 1 root root 1080 Nov  9 00:05 stop-master.sh
-rwxr-xr-x 1 root root 1227 Nov  9 00:05 stop-mesos-dispatcher.sh
-rwxr-xr-x 1 root root 1084 Nov  9 00:05 stop-mesos-shuffle-service.sh
-rwxr-xr-x 1 root root 1067 Nov  9 00:05 stop-shuffle-service.sh
-rwxr-xr-x 1 root root 1557 Nov  9 00:05 stop-slave.sh
-rwxr-xr-x 1 root root 1064 Nov  9 00:05 stop-slaves.sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可见，没有Thrift Server的启动脚本。&lt;/p&gt;
&lt;p&gt;借鉴网上资料（CDH 6成功启动spark-thrift服务 &lt;a href=&#34;https://blog.csdn.net/qq_34864753/article/details/102729859&#34;&gt;https://blog.csdn.net/qq_34864753/article/details/102729859&lt;/a&gt;），在不修改CDH Spark的前提下，我们需要启动一个独立的Spark Thrift Server。&lt;/p&gt;
&lt;p&gt;还需要考虑CDH Kerberos认证的问题。&lt;/p&gt;
&lt;h2 id=&#34;spark-thrift-server-简介&#34;&gt;Spark Thrift Server 简介&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;../../images/sparksqlthriftserver.png&#34; alt=&#34;Spark SQL Thrift Server&#34;&gt;&lt;/p&gt;
&lt;p&gt;Spark Thrift Server是Spark社区基于HiveServer2实现的一个Thrift服务。旨在无缝兼容HiveServer2。&lt;/p&gt;
&lt;p&gt;因为Spark Thrift Server的接口和协议都和HiveServer2完全一致，因此我们部署好Spark Thrift Server后，可以直接使用hive的beeline访问Spark Thrift Server执行相关语句。&lt;/p&gt;
&lt;p&gt;Spark Thrift Server的目的也只是取代HiveServer2，因此它依旧可以和Hive Metastore进行交互，获取到hive的元数据。&lt;/p&gt;
&lt;p&gt;参考 &lt;a href=&#34;https://www.jianshu.com/p/b719c6415411&#34;&gt;https://www.jianshu.com/p/b719c6415411&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;apache-spark配置&#34;&gt;Apache Spark配置&lt;/h2&gt;
&lt;h3 id=&#34;下载官方版本&#34;&gt;下载官方版本&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cd /opt
wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz
cd spark-2.4.0-bin-hadoop2.7 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; ln -s &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;pwd&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt; /opt/spark
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;spark配置文件&#34;&gt;Spark配置文件&lt;/h3&gt;
&lt;p&gt;通过软链接的方式复用Hive的配置。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;ln -s /etc/hive/conf/hive-site.xml /opt/spark/conf/hive-site.xml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;最后配置文件夹长这样。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# ll /opt/spark/conf/&lt;/span&gt;
-rw-r--r-- &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; webapp webapp  &lt;span style=&#34;color:#ae81ff&#34;&gt;996&lt;/span&gt; Oct &lt;span style=&#34;color:#ae81ff&#34;&gt;29&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;2018&lt;/span&gt; docker.properties.template
-rw-r--r-- &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; webapp webapp &lt;span style=&#34;color:#ae81ff&#34;&gt;1105&lt;/span&gt; Oct &lt;span style=&#34;color:#ae81ff&#34;&gt;29&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;2018&lt;/span&gt; fairscheduler.xml.template
lrwxrwxrwx &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; root   root     &lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt; Apr  &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt; 11:05 hive-site.xml -&amp;gt; /etc/hive/conf/hive-site.xml
-rw-r--r-- &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; webapp webapp &lt;span style=&#34;color:#ae81ff&#34;&gt;2025&lt;/span&gt; Oct &lt;span style=&#34;color:#ae81ff&#34;&gt;29&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;2018&lt;/span&gt; log4j.properties.template
-rw-r--r-- &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; webapp webapp &lt;span style=&#34;color:#ae81ff&#34;&gt;7801&lt;/span&gt; Oct &lt;span style=&#34;color:#ae81ff&#34;&gt;29&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;2018&lt;/span&gt; metrics.properties.template
-rw-r--r-- &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; webapp webapp  &lt;span style=&#34;color:#ae81ff&#34;&gt;865&lt;/span&gt; Oct &lt;span style=&#34;color:#ae81ff&#34;&gt;29&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;2018&lt;/span&gt; slaves.template
-rw-r--r-- &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; webapp webapp &lt;span style=&#34;color:#ae81ff&#34;&gt;1292&lt;/span&gt; Oct &lt;span style=&#34;color:#ae81ff&#34;&gt;29&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;2018&lt;/span&gt; spark-defaults.conf.template
-rwxr-xr-x &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; webapp webapp &lt;span style=&#34;color:#ae81ff&#34;&gt;4221&lt;/span&gt; Oct &lt;span style=&#34;color:#ae81ff&#34;&gt;29&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;2018&lt;/span&gt; spark-env.sh.template
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;spark配置文件配置方式&#34;&gt;Spark配置文件配置方式&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;默认配置路径：&lt;code&gt;$SPARK_HOME/conf&lt;/code&gt;，如果&lt;code&gt;$SPARK_HOME&lt;/code&gt;不存在，脚本中会把脚本上层目录当做&lt;code&gt;$SPARK_HOME&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$SPARK_CONF_DIR&lt;/code&gt;，可以指定SPARK配置文件夹。&lt;/li&gt;
&lt;li&gt;Spark classpath，如果&lt;code&gt;hdfs-site.xml&lt;/code&gt; &lt;code&gt;core-site.xml&lt;/code&gt;在classpath，Spark可以读取。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$HADOOP_CONF_DIR&lt;/code&gt;，一般是&lt;code&gt;/etc/hadoop/conf&lt;/code&gt;目录，读Hadoop配置信息。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$YARN_CONF_DIR&lt;/code&gt;，一般也是&lt;code&gt;/etc/hadoop/conf&lt;/code&gt;目录。&lt;/li&gt;
&lt;li&gt;命令行中可以覆盖以上配置文件中的具体参数。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;参考 &lt;a href=&#34;https://spark.apache.org/docs/latest/configuration.html#inheriting-hadoop-cluster-configuration&#34;&gt;https://spark.apache.org/docs/latest/configuration.html#inheriting-hadoop-cluster-configuration&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;spark-lib修改&#34;&gt;Spark lib修改&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cd /opt/spark
rm -rf jars/hadoop-yarn-*
cp /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/hadoop-yarn-* jars/
cp /opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/hive-shims-scheduler-2.1.1-cdh6.3.2.jar jars/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;kerberos-认证&#34;&gt;Kerberos 认证&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200410140136660.png&#34; alt=&#34;image-20200410140136660&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;图中的三个角色，在Kerkeros认证体系下都对应一个principal。Kerberos principal相当于用户名，keytab相当于密码。权限配置，依靠hive与hdfs本身。&lt;/li&gt;
&lt;li&gt;JDBC客户端与Spark JDBC Server需要使用Kerberos认证。JDBC客户端需要拥有principal/keytab对。我们手动创建。&lt;/li&gt;
&lt;li&gt;Spark JDBC Server与Hive metastore需要使用Kerberos认证。JDBC服务端需要拥有principal/keytab对。我们手动创建。&lt;/li&gt;
&lt;li&gt;Hive metastore也拥有自己的principal/keytab对，不过这个已经由CDH托管了。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;常见Kerberos错误：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;org.apache.hive.service.ServiceException: Unable to login to kerberos with given principal/keytab / Caused by: java.io.IOException: HiveServer2 Kerberos principal or keytab is not correctly configured&lt;/li&gt;
&lt;li&gt;Caused by: java.io.IOException: Login failure for &lt;a href=&#34;mailto:hive/xh-hd2-peggy-dost000003@PEGGY.LING&#34;&gt;hive/xh-hd2-peggy-dost000003@PEGGY.LING&lt;/a&gt; from keytab hive.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user&lt;/li&gt;
&lt;li&gt;SASL negotiation failure
javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;创建-spark-jdbc-server-的-principal&#34;&gt;创建 Spark JDBC Server 的 Principal&lt;/h3&gt;
&lt;p&gt;因为复用了Hive的配置文件，待创建的principal的名字需要满足配置中的规范。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;webapp@xh-hd2-peggy-dost000004 spark&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;$ cat conf/hive-site.xml
...
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hive.server2.authentication.kerberos.principal&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;hive/_HOST@PEGGY.LING&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
... 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在Kerberos服务器创建 principal及导出 keytab，同步到Spark JDBC Server所在机器。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# create principal&lt;/span&gt;
kadmin.local addprinc -randkey hive/xh-hd2-peggy-dost000004
&lt;span style=&#34;color:#75715e&#34;&gt;# export keytab&lt;/span&gt;
kadmin.local ktadd -k hive.xh-hd2-peggy-dost000004.keytab hive/xh-hd2-peggy-dost000004
&lt;span style=&#34;color:#75715e&#34;&gt;# 验证是否OK&lt;/span&gt;
kinit -kt hive.xh-hd2-peggy-dost000004.keytab hive/xh-hd2-peggy-dost000004@PEGGY.LING
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;启动-spark-jdbc-server&#34;&gt;启动 Spark JDBC Server&lt;/h3&gt;
&lt;p&gt;启动脚本如下。因为配置文件里没有指定keytab的路径，需要通过&lt;code&gt;--hiveconf&lt;/code&gt;指定。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#!/usr/bin/bash
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
export JAVA_HOME&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/usr/java/jdk1.8.0_181-cloudera
export PATH&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;$PATH:$JAVA_HOME/bin
export HADOOP_CONF_DIR&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/etc/hadoop/conf

kinit -kt hive.xh-hd2-peggy-dost000004.keytab hive/xh-hd2-peggy-dost000004@PEGGY.LING

sbin/start-thriftserver.sh &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--hiveconf hive.server2.authentication.kerberos.keytab hive.xh-hd2-peggy-dost000004.keytab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;也可以参考这篇文章：&lt;/p&gt;
&lt;p&gt;Configuring Spark Thrift Server with Kerberos &lt;a href=&#34;https://mapr.com/docs/61/Spark/ConfiguringSparkSQLThriftServer_Kerberos.html&#34;&gt;https://mapr.com/docs/61/Spark/ConfiguringSparkSQLThriftServer_Kerberos.html&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;kinit-ticket过期问题&#34;&gt;kinit ticket过期问题&lt;/h4&gt;
&lt;p&gt;Spark Thrift Server 进程会自动处理Kerberos ticket renewal操作。&lt;/p&gt;
&lt;p&gt;默认的ticket_lifetime 1d，renew_lifetime 7d。上次kinit是04/10。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[webapp@xh-hd2-peggy-dost000004 spark]$ klist
Ticket cache: FILE:/tmp/krb5cc_1000
Default principal: hive/xh-hd2-peggy-dost000004@PEGGY.LING

Valid starting       Expires              Service principal
04/13/2020 01:32:45  04/14/2020 01:32:45  krbtgt/PEGGY.LING@PEGGY.LING
	renew until 04/17/2020 15:56:45
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;就看7天后会怎么样了。renew周期过了，会重建么？理论上是可以的。&lt;/p&gt;
&lt;p&gt;看到Spark代码中已经有kerberos集成了，包括登录啥的。&lt;/p&gt;
&lt;p&gt;spark/sql/hive-thriftserver/v2.3.5/src/main/java/org/apache/hive/service/auth/HiveAuthFactory.java&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200413110032646.png&#34; alt=&#34;image-20200413110032646&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200413105917167.png&#34; alt=&#34;image-20200413105917167&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;创建-jdbc-client-的-principal&#34;&gt;创建 JDBC Client 的 Principal&lt;/h3&gt;
&lt;p&gt;在Kerberos服务器创建 principal及导出 keytab，同步到 JDBC Client 所在机器。这里对principal的名称没有严格要求。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# create principal&lt;/span&gt;
kadmin.local addprinc -randkey hive/xh-hd2-peggy-dost000003
&lt;span style=&#34;color:#75715e&#34;&gt;# export keytab&lt;/span&gt;
kadmin.local ktadd -k hive.xh-hd2-peggy-dost000003.keytab hive/xh-hd2-peggy-dost000003 
&lt;span style=&#34;color:#75715e&#34;&gt;# 验证是否OK&lt;/span&gt;
kinit -kt hive.xh-hd2-peggy-dost000003.keytab hive/xh-hd2-peggy-dost000003@PEGGY.LING
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;启动-jdbc-client&#34;&gt;启动 JDBC Client&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;webapp@xh-hd2-peggy-dost000003 spark&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;$ kinit -kt hive.xh-hd2-peggy-dost000003.keytab hive/xh-hd2-peggy-dost000003@PEGGY.LING

&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;webapp@xh-hd2-peggy-dost000003 spark&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;$ bin/beeline -u &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;jdbc:hive2://xh-hd2-peggy-dost000004:10000/default;principal=hive/xh-hd2-peggy-dost000004@PEGGY.LING&amp;#34;&lt;/span&gt;
Connecting to jdbc:hive2://xh-hd2-peggy-dost000004:10000/default;principal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;hive/xh-hd2-peggy-dost000004@PEGGY.LING
2020-04-10 16:09:01 INFO  Utils:310 - Supplied authorities: xh-hd2-peggy-dost000004:10000
2020-04-10 16:09:01 INFO  Utils:397 - Resolved authority: xh-hd2-peggy-dost000004:10000
2020-04-10 16:09:01 INFO  HiveConnection:203 - Will try to open client transport with JDBC Uri: jdbc:hive2://xh-hd2-peggy-dost000004:10000/default;principal&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;hive/xh-hd2-peggy-dost000004@PEGGY.LING
Connected to: Spark SQL &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;version 2.4.0&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
Driver: Hive JDBC &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;version 1.2.1.spark2&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 1.2.1.spark2 by Apache Hive
0: jdbc:hive2://xh-hd2-peggy-dost000004:10000&amp;gt; show databases;
+---------------+--+
| databaseName  |
+---------------+--+
| db1           |
| default       |
| product       |
+---------------+--+
&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; rows selected &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;0.091 seconds&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;至此，Thrift Server的启用就完成了。&lt;/p&gt;
&lt;h3 id=&#34;yarn-运行&#34;&gt;YARN 运行&lt;/h3&gt;
&lt;p&gt;失败了，TODO&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sbin/start-thriftserver.sh --hiveconf hive.server2.authentication.kerberos.keytab hive.keytab --hiveconf hive.server2.thrift.port=10001 --queue root.zm_yarn_pool.development  --master yarn --executor-memory 4g --executor-cores 2 --num-executors 20

2020-04-09 17:56:19 INFO  Client:54 - Requesting a new application from cluster with 5 NodeManagers
Exception in thread &amp;quot;main&amp;quot; java.lang.NoClassDefFoundError: org/apache/hadoop/util/FastNumberFormat
        at org.apache.hadoop.yarn.api.records.ApplicationId.toString(ApplicationId.java:104)
        at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getAppStagingDir(Client.scala:1222)
        at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$cleanupStagingDirInternal$1(Client.scala:206)
        at org.apache.spark.deploy.yarn.Client.cleanupStagingDir(Client.scala:226)
        at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:191)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:178)
        at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:501)
        at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2520)
        at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:935)
        at org.apache.spark.sql.SparkSession$Builder$$anonfun$7.apply(SparkSession.scala:926)
        at scala.Option.getOrElse(Option.scala:121)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;外传cdh是如何管理hadoop配置文件的&#34;&gt;【外传】CDH是如何管理Hadoop配置文件的&lt;/h2&gt;
&lt;p&gt;以下结果都是通过观察实践所得。&lt;/p&gt;
&lt;p&gt;以Hive为例，&lt;code&gt;/etc/hive/conf&lt;/code&gt;下的配置文件由CDH生成。（CDH管理界面上可以修改这些配置。）&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# ll /etc/hive/conf/&lt;/span&gt;
total &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;
-rw-r--r-- &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; root root   &lt;span style=&#34;color:#ae81ff&#34;&gt;21&lt;/span&gt; Dec &lt;span style=&#34;color:#ae81ff&#34;&gt;31&lt;/span&gt; 15:02 __cloudera_generation__
-rw-r--r-- &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; root root   &lt;span style=&#34;color:#ae81ff&#34;&gt;70&lt;/span&gt; Dec &lt;span style=&#34;color:#ae81ff&#34;&gt;31&lt;/span&gt; 15:02 __cloudera_metadata__
-rw-r--r-- &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; root root &lt;span style=&#34;color:#ae81ff&#34;&gt;3846&lt;/span&gt; Dec &lt;span style=&#34;color:#ae81ff&#34;&gt;31&lt;/span&gt; 15:02 core-site.xml
-rw-r--r-- &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; root root  &lt;span style=&#34;color:#ae81ff&#34;&gt;617&lt;/span&gt; Dec &lt;span style=&#34;color:#ae81ff&#34;&gt;31&lt;/span&gt; 15:02 hadoop-env.sh
-rw-r--r-- &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; root root &lt;span style=&#34;color:#ae81ff&#34;&gt;3839&lt;/span&gt; Dec &lt;span style=&#34;color:#ae81ff&#34;&gt;31&lt;/span&gt; 15:02 hdfs-site.xml
-rw-r--r-- &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; root root &lt;span style=&#34;color:#ae81ff&#34;&gt;2655&lt;/span&gt; Dec &lt;span style=&#34;color:#ae81ff&#34;&gt;31&lt;/span&gt; 15:02 hive-env.sh
-rw-r--r-- &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; root root &lt;span style=&#34;color:#ae81ff&#34;&gt;6925&lt;/span&gt; Dec &lt;span style=&#34;color:#ae81ff&#34;&gt;31&lt;/span&gt; 15:02 hive-site.xml
...

&lt;span style=&#34;color:#75715e&#34;&gt;# head /etc/hive/conf/hive-site.xml&lt;/span&gt;
&amp;lt;?xml version&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1.0&amp;#34;&lt;/span&gt; encoding&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;UTF-8&amp;#34;&lt;/span&gt;?&amp;gt;

&amp;lt;!--Autogenerated by Cloudera Manager--&amp;gt;
&amp;lt;configuration&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hive.metastore.uris&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;thrift://xh-hd2-peggy-dost001:9083&amp;lt;/value&amp;gt;
  &amp;lt;/property&amp;gt;
  &amp;lt;property&amp;gt;
    &amp;lt;name&amp;gt;hive.metastore.client.socket.timeout&amp;lt;/name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我们的集群是开启了Kerberos认证的。但是上述配置文件里没见principal的keytab路径配置。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# grep keytab /etc/hive/conf/hive-site.xml&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;也就是说，&lt;code&gt;/etc/hive/conf&lt;/code&gt;不是真正在被使用的配置文件。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;/var/run/cloudera-scm-agent/process/&lt;/code&gt; 这个目录中有所有CDH监控的进程，包括我们关注的Hive进程。&lt;/p&gt;
&lt;p&gt;参考 &lt;a href=&#34;https://community.cloudera.com/t5/Support-Questions/Location-of-keytab-files/td-p/33716&#34;&gt;https://community.cloudera.com/t5/Support-Questions/Location-of-keytab-files/td-p/33716&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ls /var/run/cloudera-scm-agent/process/*hive*
/var/run/cloudera-scm-agent/process/954-hive-HIVEMETASTORE:
cloudera-monitor.properties        config.zip     creds.localjceks  hdfs-site.xml  hive-log4j2.properties  logs       redaction-rules.json  service-metrics.properties  supervisor_status
cloudera-stack-monitor.properties  core-site.xml  exit_code         hive.keytab    hive-site.xml           proc.json  sentry-site.xml       supervisor.conf             yarn-conf

/var/run/cloudera-scm-agent/process/955-hive-HIVESERVER2:
cloudera-monitor.properties        config.zip     exit_code           hdfs-site.xml  hive-log4j2.properties  logs                         navigator.lineage.client.properties  redaction-rules.json  service-metrics.properties  supervisor.conf    yarn-conf
cloudera-stack-monitor.properties  core-site.xml  fair-scheduler.xml  hive.keytab    hive-site.xml           navigator.client.properties  proc.json                            sentry-site.xml       spark-defaults.conf         supervisor_status

/var/run/cloudera-scm-agent/process/956-hive-WEBHCAT:
cloudera-monitor.properties        config.zip     exit_code      hive-site.xml  logs       redaction-rules.json        supervisor.conf    webhcat-default.xml       webhcat-site.xml
cloudera-stack-monitor.properties  core-site.xml  hdfs-site.xml  HTTP.keytab    proc.json  service-metrics.properties  supervisor_status  webhcat-log4j.properties  yarn-conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;这里Hive有三个进程，metastore，hiveserver2，WEBHCAT。可见principal对应的keytab，这里的 &lt;code&gt;hive.keytab&lt;/code&gt;，也是在这里维护着。配置文件也是在&lt;code&gt;/etc/hive/conf&lt;/code&gt;基础上作了改动，比如keytab路径的设置。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;root 954-hive-HIVEMETASTORE&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# grep kerberos hive-site.xml&lt;/span&gt;
    &amp;lt;name&amp;gt;hive.metastore.kerberos.principal&amp;lt;/name&amp;gt;
    &amp;lt;name&amp;gt;hive.metastore.kerberos.keytab.file&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;kerberos&amp;lt;/value&amp;gt;
    
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;root 955-hive-HIVESERVER2&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# grep kerberos hive-site.xml&lt;/span&gt;
    &amp;lt;value&amp;gt;kerberos&amp;lt;/value&amp;gt;
    &amp;lt;name&amp;gt;hive.metastore.kerberos.principal&amp;lt;/name&amp;gt;
    &amp;lt;name&amp;gt;hive.server2.authentication.kerberos.principal&amp;lt;/name&amp;gt;
    &amp;lt;name&amp;gt;hive.server2.authentication.kerberos.keytab&amp;lt;/name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;HiveMetaStore与HiveServer2使用的keytab是不同的。一个principal对应多个keytab么？TODO：可能是因为加密随机的原因？？？&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;root@xh-hd2-peggy-dost001 955-hive-HIVESERVER2&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# md5sum hive.keytab&lt;/span&gt;
523357eec4f542b7b3df7ec52cee43b2  hive.keytab

&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;root@xh-hd2-peggy-dost001 954-hive-HIVEMETASTORE&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# md5sum hive.keytab&lt;/span&gt;
3c6f52333067518ae4bdce1e99878857  hive.keytab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;TODO：实验发现，导出两次，之前的keytab就失效了！进入知识盲区。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[root@xh-hd2-peggy-rost01 ~]# kadmin.local addprinc -randkey hive/xh-hd2-peggy-dost000003
[root@xh-hd2-peggy-rost01 ~]# kadmin.local ktadd -k hive.xh-hd2-peggy-dost000003.keytab hive/xh-hd2-peggy-dost000003
[root@xh-hd2-peggy-rost01 ~]# mv hive.xh-hd2-peggy-dost000003.keytab hive.xh-hd2-peggy-dost000003.keytab.bak
[root@xh-hd2-peggy-rost01 ~]# kadmin.local ktadd -k hive.xh-hd2-peggy-dost000003.keytab hive/xh-hd2-peggy-dost000003
[root@xh-hd2-peggy-rost01 ~]# kinit -kt hive.xh-hd2-peggy-dost000003.keytab hive/xh-hd2-peggy-dost000003@PEGGY.LING
[root@xh-hd2-peggy-rost01 ~]# kinit -kt hive.xh-hd2-peggy-dost000003.keytab.bak hive/xh-hd2-peggy-dost000003@PEGGY.LING
kinit: Password incorrect while getting initial credentials
&lt;/code&gt;&lt;/pre&gt;- https://xujiahua.github.io/posts/20200410-spark-thrift-server-cdh/ - </description>
        </item>
    
    
    
        <item>
        <title>Docker日志驱动小结</title>
        <link>https://xujiahua.github.io/posts/20200403-docker-logging/</link>
        <pubDate>Fri, 03 Apr 2020 15:43:07 +0800</pubDate>
        
        <guid>https://xujiahua.github.io/posts/20200403-docker-logging/</guid>
        <description>许嘉华的博客 https://xujiahua.github.io/posts/20200403-docker-logging/ -&lt;p&gt;&lt;code&gt;docker logs&lt;/code&gt;， &lt;code&gt;kubectl logs&lt;/code&gt;能看到Docker容器的标准输出、标准错误，方便定位问题。而 &lt;code&gt;xxx logs&lt;/code&gt;之所以能看到，是因为标准输出、标准错误存储在每个容器独有的日志文件中。&lt;/p&gt;
&lt;p&gt;另外日志量大了，用&lt;code&gt;docker logs&lt;/code&gt;看历史数据不大合适。我们就需要考虑将日志存储到日志中心去。&lt;/p&gt;
&lt;p&gt;Docker默认支持如下日志驱动。有直接写文件的，有使用云服务的。下面简单介绍下。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/Screen-Shot-2017-09-11-at-3.08.50-PM.png&#34; alt=&#34;img&#34;&gt;&lt;/p&gt;
&lt;p&gt;credit: &lt;a href=&#34;https://jaxenter.com/docker-logging-gotchas-137049.html&#34;&gt;https://jaxenter.com/docker-logging-gotchas-137049.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;官方文档 &lt;a href=&#34;https://docs.docker.com/config/containers/logging/configure/&#34;&gt;https://docs.docker.com/config/containers/logging/configure/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;默认驱动json-file&#34;&gt;默认驱动：json-file&lt;/h2&gt;
&lt;p&gt;默认的Logging Driver是json-file。&lt;code&gt;docker info&lt;/code&gt;可以查看。全局的日志驱动设置，可以修改daemon配置文件 &lt;code&gt;/etc/docker/daemon.json&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;写入文件的日志格式长这样：&lt;code&gt;{&amp;quot;log&amp;quot;:&amp;quot;Log line is here\n&amp;quot;,&amp;quot;stream&amp;quot;:&amp;quot;stdout&amp;quot;,&amp;quot;time&amp;quot;:&amp;quot;2019-01-01T11:11:11.111111111Z&amp;quot;}&lt;/code&gt;，每一行是一个json文件，log字段为容器原来输出的每行内容。&lt;/p&gt;
&lt;p&gt;默认配置，创建的容器的信息在这个目录下： &lt;code&gt;/var/lib/docker/containers&lt;/code&gt;。&lt;/p&gt;
&lt;h3 id=&#34;实验&#34;&gt;实验&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;root@ubuntu-parallel:~# docker run --name default_logging_driver hello-world

root@ubuntu-parallel:~# cd /var/lib/docker/containers/&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;docker ps --no-trunc -aqf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;name=default_logging_driver&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;

root@ubuntu-parallel:~# cat &lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;docker ps --no-trunc -aqf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;name=default_logging_driver&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;-json.log
&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;log&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;\n&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stream&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stdout&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;time&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2020-04-02T01:46:54.096347888Z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;log&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hello from Docker!\n&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stream&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stdout&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;time&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2020-04-02T01:46:54.096377382Z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;log&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;This message shows that your installation appears to be working correctly.\n&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stream&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stdout&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;time&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2020-04-02T01:46:54.096381118Z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;log&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;\n&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stream&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;stdout&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;time&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2020-04-02T01:46:54.096383725Z&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/config/containers/logging/json-file/&#34;&gt;https://docs.docker.com/config/containers/logging/json-file/&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;怎么记录更多上下文信息&#34;&gt;怎么记录更多上下文信息&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;json-file&lt;/code&gt;本身是没有记录上下文信息的。集中存储到日志中心服务器，就无法区分具体是哪个应用产生的日志了。&lt;/p&gt;
&lt;p&gt;k8s的容器日志收集，上下文信息是由日志收集工具 &lt;code&gt;fluentd&lt;/code&gt; 通过请求api server采集并缓存起来的。参考 &lt;a href=&#34;https://xujiahua.github.io/posts/20200414-k8s-logging/&#34;&gt;https://xujiahua.github.io/posts/20200414-k8s-logging/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;同样的思路，&lt;code&gt;fluentd&lt;/code&gt;也有不少通过docker daemon查询或是解析容器目录下&lt;code&gt;config.v2.json&lt;/code&gt;获取metadata的 filter 插件。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200414112709859.png&#34; alt=&#34;image-20200414112709859&#34;&gt;&lt;/p&gt;
&lt;p&gt;参考 &lt;a href=&#34;https://www.fluentd.org/plugins&#34;&gt;https://www.fluentd.org/plugins&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;比如这个 &lt;a href=&#34;https://github.com/zsoltf/fluent-plugin-docker_metadata_elastic_filter&#34;&gt;https://github.com/zsoltf/fluent-plugin-docker_metadata_elastic_filter&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;log&amp;quot;: &amp;quot;2015/05/05 19:54:41 \n&amp;quot;,
  &amp;quot;stream&amp;quot;: &amp;quot;stderr&amp;quot;,
  &amp;quot;docker&amp;quot;: {
    &amp;quot;id&amp;quot;: &amp;quot;df14e0d5ae4c07284fa636d739c8fc2e6b52bc344658de7d3f08c36a2e804115&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;k8s_fabric8-console-container.efbd6e64_fabric8-console-controller-9knhj_default_8ae2f621-f360-11e4-8d12-54ee7527188d_7ec9aa3e&amp;quot;,
    &amp;quot;container_hostname&amp;quot;: &amp;quot;fabric8-console-controller-9knhj&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;fabric8/hawtio-kubernetes:latest&amp;quot;,
    &amp;quot;image_id&amp;quot;: &amp;quot;b2bd1a24a68356b2f30128e6e28e672c1ef92df0d9ec01ec0c7faea5d77d2303&amp;quot;,
    &amp;quot;labels&amp;quot;: {}
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;新增了docker结构体，镜像名称也能收集到了 。&lt;/p&gt;
&lt;h2 id=&#34;local&#34;&gt;local&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;--log-driver&lt;/code&gt;指定日志驱动。&lt;/p&gt;
&lt;p&gt;cat输出local文件，部分结果乱码。挺不方便日志解析的。&lt;/p&gt;
&lt;h3 id=&#34;实验-1&#34;&gt;实验&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;root@ubuntu-parallel:~# docker run --name local_logging_driver --log-driver local hello-world

root@ubuntu-parallel:~# cd /var/lib/docker/containers/&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;docker ps --no-trunc -aqf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;name=local_logging_driver&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;

root@ubuntu-parallel:~# cat local-logs/container.log
stdout�������&amp;amp;
stdout�������Hello from Docker!&amp;amp;^
stdout˧�����JThis message shows that your installation appears to be working correctly.^
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/config/containers/logging/local/&#34;&gt;https://docs.docker.com/config/containers/logging/local/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;none&#34;&gt;none&lt;/h2&gt;
&lt;p&gt;不生成日志文件，&lt;code&gt;docker logs&lt;/code&gt;也拿不到日志。实际使用不会考虑。&lt;/p&gt;
&lt;h3 id=&#34;实验-2&#34;&gt;实验&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;root@ubuntu-parallel:~# docker run --name none_logging_driver --log-driver none hello-world

root@ubuntu-parallel:~# cd /var/lib/docker/containers/&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;docker ps --no-trunc -aqf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;name=none_logging_driver&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;

root@ubuntu-parallel:~# docker logs none_logging_driver
Error response from daemon: configured logging driver does not support reading
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;syslog&#34;&gt;syslog&lt;/h2&gt;
&lt;p&gt;因为日志被写入了syslog，并混在其他应用的日志中，&lt;code&gt;docker logs&lt;/code&gt;没办法工作了。&lt;/p&gt;
&lt;h3 id=&#34;实验-3&#34;&gt;实验&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 观察syslog&lt;/span&gt;
root@ubuntu-parallel:~# tail -f /var/log/syslog

root@ubuntu-parallel:~# docker run --name syslog_logging_driver --log-driver syslog hello-world

&lt;span style=&#34;color:#75715e&#34;&gt;# 日志不会写本地&lt;/span&gt;
root@ubuntu-parallel:~# cd /var/lib/docker/containers/&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;docker ps --no-trunc -aqf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;name=syslog_logging_driver&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;

root@ubuntu-parallel:~# docker logs syslog_logging_driver
Error response from daemon: configured logging driver does not support reading
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/config/containers/logging/syslog/&#34;&gt;https://docs.docker.com/config/containers/logging/syslog/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;journald&#34;&gt;journald&lt;/h2&gt;
&lt;p&gt;写入syslog和journald，应用日志与系统日志混在一起，难以辨认了。&lt;/p&gt;
&lt;p&gt;倒是journald驱动下，可以使用&lt;code&gt;docker logs&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;参考：https://wiki.archlinux.org/index.php/Systemd/Journal&lt;/p&gt;
&lt;h3 id=&#34;实验-4&#34;&gt;实验&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;root@ubuntu-parallel:~# docker run --name journald_logging_driver --log-driver journald hello-world

root@ubuntu-parallel:~# journalctl
Apr &lt;span style=&#34;color:#ae81ff&#34;&gt;02&lt;/span&gt; 10:30:36 ubuntu-parallel 4b948bf091a8&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;999&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;: To try something more ambitious, you can run an Ubuntu container with:
Apr &lt;span style=&#34;color:#ae81ff&#34;&gt;02&lt;/span&gt; 10:30:36 ubuntu-parallel 4b948bf091a8&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;999&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;:  $ docker run -it ubuntu bash
Apr &lt;span style=&#34;color:#ae81ff&#34;&gt;02&lt;/span&gt; 10:30:36 ubuntu-parallel 4b948bf091a8&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;999&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;:
Apr &lt;span style=&#34;color:#ae81ff&#34;&gt;02&lt;/span&gt; 10:30:36 ubuntu-parallel 4b948bf091a8&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;999&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;: Share images, automate workflows, and more with a free Docker ID:
Apr &lt;span style=&#34;color:#ae81ff&#34;&gt;02&lt;/span&gt; 10:30:36 ubuntu-parallel 4b948bf091a8&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;999&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;:  https://hub.docker.com/

root@ubuntu-parallel:~# cd /var/lib/docker/containers/&lt;span style=&#34;color:#66d9ef&#34;&gt;$(&lt;/span&gt;docker ps --no-trunc -aqf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;name=journald_logging_driver&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;)&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# docker logs管用&lt;/span&gt;
root@ubuntu-parallel:~# docker logs journald_logging_driver

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/config/containers/logging/journald/&#34;&gt;https://docs.docker.com/config/containers/logging/journald/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;fluentd&#34;&gt;Fluentd&lt;/h2&gt;
&lt;p&gt;通过服务请求，让docker吐日志到fluentd进程。https://docs.docker.com/config/containers/logging/fluentd/&lt;/p&gt;
&lt;p&gt;使用包括fluentd在很多日志驱动，因为日志写入到远程服务器，会导致&lt;code&gt;docker logs&lt;/code&gt;， &lt;code&gt;kubectl logs&lt;/code&gt;不可用。&lt;/p&gt;
&lt;p&gt;Fluentd是一个挺灵活的工具，可以让fluentd主动监听容器目录下的日志文件。参考另一篇文章 &lt;a href=&#34;https://xujiahua.github.io/posts/use-fluentd/&#34;&gt;https://xujiahua.github.io/posts/use-fluentd/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;为了兼容可使用&lt;code&gt;docker logs&lt;/code&gt; ，&lt;code&gt;kubectl logs&lt;/code&gt;，必须使用写本地文件的日志驱动。而json格式更方便工具（比如fluentd，logstash）解析，所以json-file是首选。&lt;/p&gt;
&lt;p&gt;然后使用日志收集工具集中采集docker容器日志。k8s中日志收集策略，一般是在每台服务器上以DaemonSet的形式安装logging agent，监听本地文件、文件夹，将日志转发到日志中心。&lt;/p&gt;
&lt;p&gt;当然这个前提条件是，应用日志是输出到标准输出和标准错误的。这对应用日志的规范有一定要求：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;不输出多行日志。比如panic、exception。&lt;/li&gt;
&lt;li&gt;应用日志使用JSON格式输出，方便后续的日志分析。&lt;/li&gt;
&lt;li&gt;应用日志中加入更多的上下文信息。用于问题定位，维度分析。&lt;/li&gt;
&lt;li&gt;Go应用开发，使用logrus日志库，加字段，以JSON格式输出都很方便。&lt;/li&gt;
&lt;li&gt;应用不关注日志该如何收集这个问题。不在应用层写日志到kafka、redis等中间件，让基础设施层处理。&lt;/li&gt;
&lt;li&gt;应用要么写入文件、要么写入标准输出，这个应该很方便做成可配置的。对程序来说，都有共同的抽象，io.Writer。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;应用日志如果是写到文件的，需要考虑通过数据卷，挂载等将日志与容器分离。采集挂载目录上的日志文件，以前怎么收集，现在还是怎么收集。还是建议写标准输出，这是目前的最佳实践。&lt;/p&gt;
- https://xujiahua.github.io/posts/20200403-docker-logging/ - </description>
        </item>
    
    
    
        <item>
        <title>Fluentd实战</title>
        <link>https://xujiahua.github.io/posts/20200402-use-fluentd/</link>
        <pubDate>Thu, 02 Apr 2020 16:58:23 +0800</pubDate>
        
        <guid>https://xujiahua.github.io/posts/20200402-use-fluentd/</guid>
        <description>许嘉华的博客 https://xujiahua.github.io/posts/20200402-use-fluentd/ -&lt;p&gt;以收集Docker容器日志的例子，介绍下Fluentd的用法。不考虑logstash，太占服务器资源了。&lt;/p&gt;
&lt;h2 id=&#34;安装-fluentd&#34;&gt;安装 Fluentd&lt;/h2&gt;
&lt;p&gt;Ubuntu 18.04上的安装命令（https://docs.fluentd.org/installation/install-by-deb）：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@ubuntu-parallel:~# curl -L https://toolbelt.treasuredata.com/sh/install-ubuntu-bionic-td-agent3.sh | sh
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以Daemon方式启动：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@ubuntu-parallel:~# systemctl start td-agent.service
root@ubuntu-parallel:~# systemctl status td-agent.service
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;fluentd的安装目录是在/opt/td-agent/下的。为演示方便，我们可以直接使用 &lt;code&gt;/opt/td-agent/embedded/bin/fluentd&lt;/code&gt;这个程序。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@ubuntu-parallel:~# ps -ef | grep fluentd
td-agent 30596     1  0 17:10 ?        00:00:00 /opt/td-agent/embedded/bin/ruby /opt/td-agent/embedded/bin/fluentd --log /var/log/td-agent/td-agent.log --daemon /var/run/td-agent/td-agent.pid
td-agent 30602 30596  9 17:10 ?        00:00:00 /opt/td-agent/embedded/bin/ruby -Eascii-8bit:ascii-8bit /opt/td-agent/embedded/bin/fluentd --log /var/log/td-agent/td-agent.log --daemon /var/run/td-agent/td-agent.pid --under-supervisor
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;其他系统的安装参考：https://docs.fluentd.org/installation&lt;/p&gt;
&lt;h3 id=&#34;小试牛刀&#34;&gt;小试牛刀&lt;/h3&gt;
&lt;p&gt;配置文件 test.conf，启动一个HTTP服务，并把接收到的日志，打印到标准输出。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;source&amp;gt;
  @type http
  port 9880
&amp;lt;/source&amp;gt;

&amp;lt;match *.*&amp;gt;
  @type stdout
&amp;lt;/match&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;启动fluentd进程。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@ubuntu-parallel:~# /opt/td-agent/embedded/bin/fluentd -c test.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;通过HTTP服务提交日志。可以看到fluentd终端打印出了输入的日志。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@ubuntu-parallel:~# curl -X POST -d &#39;json={&amp;quot;event&amp;quot;:&amp;quot;data&amp;quot;}&#39; http://localhost:9880/my.tag
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;实战收集linux主机上的docker容器日志&#34;&gt;实战：收集Linux主机上的Docker容器日志&lt;/h2&gt;
&lt;p&gt;Docker官方有fluentd的Logging Driver。不足之处：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;因为没有了本地日志文件，docker logs 不管用了。&lt;/li&gt;
&lt;li&gt;需要额外配置，或是在daemon.json里设定，或是docker run时候设定。新增机器或是运行容器，很容易忘记。&lt;/li&gt;
&lt;li&gt;如果远程数据存储down了，会不会丢日志。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;考虑到这些问题，本实验以收集本地日志的方式使用fluentd。&lt;/p&gt;
&lt;h3 id=&#34;实验前提条件&#34;&gt;实验前提条件&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Linux系统&lt;/li&gt;
&lt;li&gt;Docker容器使用默认的Logging Driver，也就是json-file&lt;/li&gt;
&lt;li&gt;容器日志目录（也包含其他配置文件的）&lt;code&gt;/var/lib/docker/containers&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;配置文件输出到stdout&#34;&gt;配置文件（输出到stdout）&lt;/h3&gt;
&lt;p&gt;定义一个配置文件 docker.container.log.conf&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;source&amp;gt;
  @type tail
  path /var/lib/docker/containers/*/*-json.log
  pos_file /var/log/docker_container.log.pos
  tag docker.*
  refresh_interval 10
  read_from_head true
  &amp;lt;parse&amp;gt;
    @type json
  &amp;lt;/parse&amp;gt;
&amp;lt;/source&amp;gt;

&amp;lt;filter docker.var.lib.docker.containers.*.*.log&amp;gt;
  @type parser
  key_name log
  remove_key_name_field true
  &amp;lt;parse&amp;gt;
    @type json
  &amp;lt;/parse&amp;gt;
&amp;lt;/filter&amp;gt;

&amp;lt;filter docker.var.lib.docker.containers.*.*.log&amp;gt;
  @type record_transformer
  &amp;lt;record&amp;gt;
    container_id ${tag_parts[5]}
    hostname &amp;quot;#{Socket.gethostname}&amp;quot;
  &amp;lt;/record&amp;gt;
&amp;lt;/filter&amp;gt;

&amp;lt;match docker.var.lib.docker.containers.*.*.log&amp;gt;
  @type stdout
&amp;lt;/match&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;简要介绍&#34;&gt;简要介绍&lt;/h4&gt;
&lt;p&gt;一个典型的日志收集过程分三部分：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;定义数据源，也就是source部分&lt;/li&gt;
&lt;li&gt;每条数据可以增删修改字段，也就是filter部分，filter可以是0个或是多个，按顺序处理。&lt;/li&gt;
&lt;li&gt;数据输出，也就是match部分&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;source&#34;&gt;source&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;@type tail 使用tail插件 &lt;a href=&#34;https://docs.fluentd.org/input/tail&#34;&gt;https://docs.fluentd.org/input/tail&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;path 定义路径，正则匹配&lt;/li&gt;
&lt;li&gt;pos_file 存放读文件的offset&lt;/li&gt;
&lt;li&gt;需要定义tag，filter、match使用tag进行匹配。&lt;/li&gt;
&lt;li&gt;tag docker.*，*会被实际的文件名给替代&lt;/li&gt;
&lt;li&gt;refresh_interval 设置多久刷新监听的文件列表，默认60秒&lt;/li&gt;
&lt;li&gt;read_from_head 默认false，也就是，没有pos存在的话，从文件尾读取。设置为true，解决新增文件而文件又没被fluentd及时监听起来的问题。&lt;/li&gt;
&lt;li&gt;parse 使用json解析，因为docker的logging driver用的json-file&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;更多source插件见：https://docs.fluentd.org/input&lt;/p&gt;
&lt;h4 id=&#34;filter&#34;&gt;filter&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;指定tag pattern&lt;/li&gt;
&lt;li&gt;@type parser 使用parser插件 &lt;a href=&#34;https://docs.fluentd.org/filter/parser&#34;&gt;https://docs.fluentd.org/filter/parser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;key_name 需要parse的字段名&lt;/li&gt;
&lt;li&gt;remove_key_name_field 因为我们只需要log字段的json内容，log字段本身不需要保留&lt;/li&gt;
&lt;li&gt;parse 解析为json，因为我们的应用日志是json格式输出的&lt;/li&gt;
&lt;li&gt;@type record_transformer 使用record_transformer插件 &lt;a href=&#34;https://docs.fluentd.org/filter/record_transformer&#34;&gt;https://docs.fluentd.org/filter/record_transformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;record 新增字段&lt;/li&gt;
&lt;li&gt;${tag_parts[5]} 预定义的变量，tag_parts是tag字符串被&amp;rdquo;.&amp;ldquo;切分后的数组，第6个代表的是container_id&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;#{Socket.gethostname}&amp;quot;&lt;/code&gt; ruby内的变量&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;更多filter插件见：https://docs.fluentd.org/filter&lt;/p&gt;
&lt;h4 id=&#34;match&#34;&gt;match&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;指定tag pattern&lt;/li&gt;
&lt;li&gt;@type stdout 使用stdout插件，用于演示 &lt;a href=&#34;https://docs.fluentd.org/output/stdout&#34;&gt;https://docs.fluentd.org/output/stdout&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;启动fluentd&#34;&gt;启动fluentd&lt;/h3&gt;
&lt;p&gt;注意查看日志。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@ubuntu-parallel:~# /opt/td-agent/embedded/bin/fluentd -c docker.container.log.conf
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;生成日志&#34;&gt;生成日志&lt;/h3&gt;
&lt;p&gt;JSON格式输出每一行日志。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@ubuntu-parallel:~# docker run -it busybox echo &#39;{&amp;quot;user&amp;quot;:1,&amp;quot;num&amp;quot;:2}&#39;
{&amp;quot;user&amp;quot;:1,&amp;quot;num&amp;quot;:2}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;观察 &lt;code&gt;/var/lib/docker/containers/*/*-json.log&lt;/code&gt;日志内容：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{&amp;quot;log&amp;quot;:&amp;quot;{\&amp;quot;user\&amp;quot;:1,\&amp;quot;num\&amp;quot;:2}\r\n&amp;quot;,&amp;quot;stream&amp;quot;:&amp;quot;stdout&amp;quot;,&amp;quot;time&amp;quot;:&amp;quot;2020-04-02T12:25:50.877037148Z&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;fluentd stdout输出：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{&amp;quot;user&amp;quot;:1,&amp;quot;num&amp;quot;:2,&amp;quot;container_id&amp;quot;:&amp;quot;e4fb94ee2d067450eeaf15837ed1497e2b7eb2f6754ba4eec1792ee37e31f12f&amp;quot;,&amp;quot;hostname&amp;quot;:&amp;quot;ubuntu-parallel&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;输出从stdout改为elasticsearch&#34;&gt;输出从stdout改为elasticsearch&lt;/h3&gt;
&lt;h4 id=&#34;启动es&#34;&gt;启动ES&lt;/h4&gt;
&lt;p&gt;参考 &lt;a href=&#34;https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-docker.html&#34;&gt;https://www.elastic.co/guide/en/elastic-stack-get-started/current/get-started-docker.html&lt;/a&gt;，使用docker-compose启动es和kibana服务。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;root@ubuntu-parallel:~# docker-compose up
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;修改match&#34;&gt;修改match&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;match docker.var.lib.docker.containers.*.*.log&amp;gt;
  @type elasticsearch
  host localhost
  port 9200
  logstash_format true
&amp;lt;/match&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;参考：https://docs.fluentd.org/output/elasticsearch&lt;/p&gt;
&lt;h4 id=&#34;启动fluentd-1&#34;&gt;启动fluentd&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;root@ubuntu-parallel:~# /opt/td-agent/embedded/bin/fluentd -c docker.container.log.es.conf
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;kibana日志分析&#34;&gt;kibana日志分析&lt;/h4&gt;
&lt;p&gt;因为ES的日志已经是JSON格式输出的，所以也不需要额外造数据了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200403001746670.png&#34; alt=&#34;image-20200403001746670&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;容器上下文信息&#34;&gt;容器上下文信息&lt;/h3&gt;
&lt;p&gt;可以考虑这个filter插件 &lt;a href=&#34;https://github.com/zsoltf/fluent-plugin-docker_metadata_elastic_filter&#34;&gt;https://github.com/zsoltf/fluent-plugin-docker_metadata_elastic_filter&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;filter docker.var.lib.docker.containers.*.*.log&amp;gt;
  type docker_metadata_elastic
&amp;lt;/filter&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;新增 docker  结构体，包含镜像名称。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;log&amp;quot;: &amp;quot;2015/05/05 19:54:41 \n&amp;quot;,
  &amp;quot;stream&amp;quot;: &amp;quot;stderr&amp;quot;,
  &amp;quot;docker&amp;quot;: {
    &amp;quot;id&amp;quot;: &amp;quot;df14e0d5ae4c07284fa636d739c8fc2e6b52bc344658de7d3f08c36a2e804115&amp;quot;,
    &amp;quot;name&amp;quot;: &amp;quot;k8s_fabric8-console-container.efbd6e64_fabric8-console-controller-9knhj_default_8ae2f621-f360-11e4-8d12-54ee7527188d_7ec9aa3e&amp;quot;,
    &amp;quot;container_hostname&amp;quot;: &amp;quot;fabric8-console-controller-9knhj&amp;quot;,
    &amp;quot;image&amp;quot;: &amp;quot;fabric8/hawtio-kubernetes:latest&amp;quot;,
    &amp;quot;image_id&amp;quot;: &amp;quot;b2bd1a24a68356b2f30128e6e28e672c1ef92df0d9ec01ec0c7faea5d77d2303&amp;quot;,
    &amp;quot;labels&amp;quot;: {}
  }
}
&lt;/code&gt;&lt;/pre&gt;- https://xujiahua.github.io/posts/20200402-use-fluentd/ - </description>
        </item>
    
    
  </channel>
</rss> 