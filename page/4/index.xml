<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>许嘉华的博客</title>
    <link>https://xujiahua.github.io/</link>
    <description>Recent content on 许嘉华的博客</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 15 Jun 2020 17:19:50 +0800</lastBuildDate>
    
        <atom:link href="https://xujiahua.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    
        <item>
        <title>Implicit：推荐系统协同过滤库的测评</title>
        <link>https://xujiahua.github.io/posts/20200211-recsys-implicit/</link>
        <pubDate>Tue, 11 Feb 2020 14:34:27 +0800</pubDate>
        
        <guid>https://xujiahua.github.io/posts/20200211-recsys-implicit/</guid>
        <description>许嘉华的博客 https://xujiahua.github.io/posts/20200211-recsys-implicit/ -&lt;h2 id=&#34;简介&#34;&gt;简介&lt;/h2&gt;
&lt;p&gt;Implicit是一个推荐系统协同过滤库。所谓协同过滤，只用到了user、item的ID和user、item交互后的评分（或是某个度量）。&lt;/p&gt;
&lt;p&gt;GitHub：https://github.com/benfred/implicit&lt;/p&gt;
&lt;h2 id=&#34;安装&#34;&gt;安装&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;pip install implicit
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;其他选择&#34;&gt;其他选择&lt;/h2&gt;
&lt;p&gt;自己照着「推荐系统实战」里的基于物品推荐的相似度公式（类似关联分析中的Lift公式），也写了一个item-item recommender。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;看推荐结果，自己写与implicit ALS有点差别，与implicit Cosine的结果差不多。&lt;/li&gt;
&lt;li&gt;纯python，没有并行，没有用C/C++，果然很慢。&lt;/li&gt;
&lt;li&gt;自己写一个的目的，除了练手，也想看看implicit的效果具体怎么样。&lt;/li&gt;
&lt;li&gt;生产使用，不建议用自己手写的，水平真的有限。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Spark Mllib怎么样。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Spark Mllib支持分布式，感觉没必要。本来Implicit单机就很强了。只要把数据导出来，导到训练机器上即可。&lt;/li&gt;
&lt;li&gt;Spark分布式，扩展性好，但是性能并没有Implicit那么好。&lt;/li&gt;
&lt;li&gt;Spark还得搭配HDFS，真的蛮重的，开发效率也没那么好。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;背后的原理&#34;&gt;背后的原理&lt;/h2&gt;
&lt;p&gt;今早根据作者文章Distance Metrics for Fun and Profit，整理了一份读后感，把之前的知识给梳理起来了。这篇文章主要介绍的是距离公式，用于K近邻的推荐算法。&lt;/p&gt;
&lt;p&gt;协同过滤算法分两类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于记忆的，K近邻算法（基于“距离”公式），建议使用BM25。优势在读后感中有写。（实际效果怎么样，还得看数据）&lt;/li&gt;
&lt;li&gt;基于模型的，矩阵分解算法，建议使用 (implicit) ALS，其变种是支持implicit dataset的。矩阵分解算法SGD，只适用于评分。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;效果，见测评图。ALS的效果是最好的了。这是在movielens100k的结果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/3D66F424-97FD-4236-8A57-98111FAF65A7.png&#34; alt=&#34;3D66F424-97FD-4236-8A57-98111FAF65A7&#34;&gt;&lt;/p&gt;
&lt;p&gt;看起来指标都很低，「推荐系统实战」里差不多这个结果。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/FBC00415-D1AE-4F22-9B86-A0B070484ACF.png&#34; alt=&#34;FBC00415-D1AE-4F22-9B86-A0B070484ACF&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;遇到的小问题&#34;&gt;遇到的小问题&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;K近邻算法的model没有model.recommend_all方法。&lt;/li&gt;
&lt;li&gt;model.recommend与model.recommend_all的返回数据结构不同，model.recommend是包含评分的，而model.recommend_all只返回ID。&lt;/li&gt;
&lt;li&gt;recommend(0)，传入第一个用户ID，与recommend_all[0]，取出第一个用户ID，两者的结果是不一样的，不管用什么算法都一样！去年12月就有这样的issue：https://github.com/benfred/implicit/issues/299，我解决了，回复了这个issue。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;猜想，因为recommend方法用到indices的原因？试着重新构建下user-item维度的sparse matrix。改完后，果然就一致了。技术原因，sparse matrix transpose后，indices是未转置前的indices，没有变化。 具体地，user_items.indices与item_users.T.indices是不同的。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/159B8A3D-4238-401B-A4D3-61E086FF9F0B.png&#34; alt=&#34;159B8A3D-4238-401B-A4D3-61E086FF9F0B&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;ALS算法在Mac上表现稀烂啊，同样的代码。ALS_Faiss还报错了。先在Linux上用吧。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Mac上的ALS效果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/409E288C-81F6-4588-8731-9849D1F3278B.png&#34; alt=&#34;409E288C-81F6-4588-8731-9849D1F3278B&#34;&gt;&lt;/p&gt;
&lt;p&gt;Linux上的ALS效果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/8DD26395-F62D-4DA8-A801-493098799456.png&#34; alt=&#34;8DD26395-F62D-4DA8-A801-493098799456&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;冷启动&#34;&gt;冷启动&lt;/h2&gt;
&lt;p&gt;一开始并没有那么多行为数据。训练出的结果肯定是挺一般的。这时候用标签推荐，运营根据经验配置标签和推荐的映射即可。&lt;/p&gt;
&lt;h2 id=&#34;考虑用户特征商品特征而不仅仅是id&#34;&gt;考虑用户特征、商品特征（而不仅仅是ID）&lt;/h2&gt;
&lt;p&gt;如果有user特征、item特征，可以使用更一般的监督学习算法。或是使用factorization machine。&lt;/p&gt;
- https://xujiahua.github.io/posts/20200211-recsys-implicit/ - </description>
        </item>
    
    
    
        <item>
        <title>一点压力测试的经验</title>
        <link>https://xujiahua.github.io/posts/20181017-perf-testing/</link>
        <pubDate>Wed, 17 Oct 2018 07:42:56 +0800</pubDate>
        
        <guid>https://xujiahua.github.io/posts/20181017-perf-testing/</guid>
        <description>许嘉华的博客 https://xujiahua.github.io/posts/20181017-perf-testing/ -&lt;p&gt;（笔记迁移 @ 2020年）&lt;/p&gt;
&lt;h2 id=&#34;一概念最基础最重要&#34;&gt;一、概念，最基础最重要&lt;/h2&gt;
&lt;h3 id=&#34;1-响应时间-tp99-tp90是什么&#34;&gt;1. 响应时间 TP99, TP90是什么&lt;/h3&gt;
&lt;p&gt;除了要看TPS，也要看请求的响应时间是否在合理的范围内。太离谱，压测该停了。&lt;/p&gt;
&lt;p&gt;响应时间的指标有最大响应时间、平均响应时间、TPXXX等。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/90-percentile1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TP – Top Percentile&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TP90 = 500ms ：90%的请求都是在500ms以内。&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-错误率&#34;&gt;2. 错误率&lt;/h3&gt;
&lt;p&gt;压测过程中发现异常或是错误，或者，错误率明显上升，这时候压测就该停了。&lt;/p&gt;
&lt;h3 id=&#34;3-吞吐量-tps&#34;&gt;3. 吞吐量 TPS&lt;/h3&gt;
&lt;p&gt;单纯看TPS是没有意义的。要结合上述两个指标，一般是0错误率，TP99在xxx ms（看具体应用场景）内的TPS。&lt;/p&gt;
&lt;p&gt;比如这么描述TPS，TP99小于100ms的前提下，系统没有错误，系统可承载的TPS是1000。&lt;/p&gt;
&lt;h3 id=&#34;4-压测案例&#34;&gt;4. 压测案例&lt;/h3&gt;
&lt;p&gt;选择生产环境中并发要求高的请求。&lt;/p&gt;
&lt;h2 id=&#34;二工具选型选最合适的&#34;&gt;二、工具选型，选最合适的&lt;/h2&gt;
&lt;p&gt;工具的选择，一要看场景，二要看自己是否趁手。&lt;/p&gt;
&lt;h3 id=&#34;ab-apache-benchmark&#34;&gt;ab (apache benchmark)&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;ab -kc 1000 -n 10000 http://www.some-site.cc/tmp/index.html&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;-k   Enables HTTP keep-alive&lt;/li&gt;
&lt;li&gt;-c   Number of concurrent requests&lt;/li&gt;
&lt;li&gt;-n   Number of total requests to make&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;不足：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;只能使用单核。本身就可能是压测的瓶颈。&lt;/li&gt;
&lt;li&gt;对于带多个步骤的压测场景无力。&lt;/li&gt;
&lt;li&gt;没有（没找到）自定义断言的能力。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;reference: &lt;a href=&#34;https://en.wikipedia.org/wiki/ApacheBench&#34;&gt;https://en.wikipedia.org/wiki/ApacheBench&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;wrk&#34;&gt;wrk&lt;/h3&gt;
&lt;p&gt;wrk小巧，性能非常好，报告直观。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;wrk -t2 -c100 -d30s -R2000 http://127.0.0.1:8080/index.html&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;-d30s: 压测时间30s&lt;/li&gt;
&lt;li&gt;-t2: 2个线程&lt;/li&gt;
&lt;li&gt;-c100: 100个&amp;quot;用户数&amp;rdquo;&lt;/li&gt;
&lt;li&gt;-R2000: constant throughput of 2000 requests per second&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;不足：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对于带多个步骤的压测场景无力。lua脚本写的很费劲。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;reference: &lt;a href=&#34;https://github.com/giltene/wrk2&#34;&gt;https://github.com/giltene/wrk2&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;jmeter&#34;&gt;jmeter&lt;/h3&gt;
&lt;p&gt;框架功能强大。个人感觉太重了，全是界面的配置。&lt;/p&gt;
&lt;h3 id=&#34;locust&#34;&gt;locust&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;使用python脚本编写案例。脚本化，方便peer review。&lt;/li&gt;
&lt;li&gt;大并发的支持：因为python本身对多核的利用不够好，提高并发量的办法是多起几个进程作为slave。&lt;/li&gt;
&lt;li&gt;统计能力满足需求。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;reference: &lt;a href=&#34;https://locust.io/&#34;&gt;https://locust.io/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;三实战中的经验&#34;&gt;三、实战中的经验&lt;/h2&gt;
&lt;p&gt;TPS压不上去怎么办。看下请求的链路，从源头开始排查瓶颈。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;压测源 -&amp;gt; （对外）网络 -&amp;gt; 负载均衡 -&amp;gt; 内部网络 -&amp;gt; 应用服务器 -&amp;gt; 数据库、共享内存&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;压测源&#34;&gt;压测源&lt;/h3&gt;
&lt;p&gt;一次http请求，客户端也是占用资源的。请求的构造与发送、返回结果的接收与解析。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/http-requests.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;根据你选用的压测框架找下瓶颈。&lt;/p&gt;
&lt;p&gt;举例：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;jmeter线程数是否不够了&lt;/li&gt;
&lt;li&gt;locust的slave进程是否都已占用100%的CPU。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;网络&#34;&gt;网络&lt;/h3&gt;
&lt;p&gt;一般内部网络不会是瓶颈。
对外网络因为带宽费用问题，手头拮据点的会使用1M带宽。再怎么测试，压的都是网络的瓶颈。&lt;/p&gt;
&lt;p&gt;建议把压力源放到内部网络内，再测试。&lt;/p&gt;
&lt;h3 id=&#34;应用服务器&#34;&gt;应用服务器&lt;/h3&gt;
&lt;p&gt;服务器的指标有这么几项：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;CPU&lt;/li&gt;
&lt;li&gt;内存&lt;/li&gt;
&lt;li&gt;磁盘IO&lt;/li&gt;
&lt;li&gt;网络IO&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;查看的方式：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;简单粗暴，登录服务器敲命令 &lt;code&gt;htop&lt;/code&gt;等&lt;/li&gt;
&lt;li&gt;插件取数据&lt;/li&gt;
&lt;li&gt;阿里云监控，如果是阿里云机器&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;四golang应用监控开发篇&#34;&gt;四、golang应用监控（开发篇）&lt;/h2&gt;
&lt;h3 id=&#34;开发者角度分析并发瓶颈&#34;&gt;开发者角度分析并发瓶颈&lt;/h3&gt;
&lt;p&gt;自己写的代码已经忘了，或是别人的代码不了解，建议通过阅读代码画出时序图。&lt;/p&gt;
&lt;h3 id=&#34;pprof火焰图查看cpu耗时内存占用&#34;&gt;pprof火焰图，查看CPU耗时、内存占用&lt;/h3&gt;
&lt;p&gt;最新版本的pprof已经支持了火焰图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../images/image-20200624075225452.png&#34; alt=&#34;image-20200624075225452&#34;&gt;&lt;/p&gt;
&lt;p&gt;火焰图的y轴表示cpu调用方法的先后，x轴表示在每个采样调用时间内，方法所占的时间百分比，越宽代表占据cpu时间越多。&lt;/p&gt;
&lt;h4 id=&#34;1-安装最新版的pprof&#34;&gt;1. 安装最新版的pprof&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;go get -u github.com/google/pprof
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;pprof具体路径为 $GOPATH/bin/pprof&lt;/p&gt;
&lt;h4 id=&#34;2-数据采用代码引用&#34;&gt;2. 数据采用代码引用&lt;/h4&gt;
&lt;p&gt;web应用，通过http接口的方式获得采样数据（非web应用也建议这么做，接口比较high level）。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;// pprof
go func() {
   r := http.NewServeMux()
   // Register pprof handlers
   r.HandleFunc(&amp;quot;/debug/pprof/&amp;quot;, pprof.Index)
   r.HandleFunc(&amp;quot;/debug/pprof/cmdline&amp;quot;, pprof.Cmdline)
   r.HandleFunc(&amp;quot;/debug/pprof/profile&amp;quot;, pprof.Profile)
   r.HandleFunc(&amp;quot;/debug/pprof/symbol&amp;quot;, pprof.Symbol)
   r.HandleFunc(&amp;quot;/debug/pprof/trace&amp;quot;, pprof.Trace)

   http.ListenAndServe(&amp;quot;:8000&amp;quot;, r)
}()
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;3-打开管理工具&#34;&gt;3. 打开管理工具&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;-- CPU
$GOPATH/bin/pprof -http=:9000 coupon http://localhost:8000/debug/pprof/profile
-- Memory
$GOPATH/bin/pprof -http=:9000 coupon http://localhost:8000/debug/pprof/heap
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;中间的&lt;code&gt;coupon&lt;/code&gt;是二进制文件。用于取symbol。发布的时候留一份copy。&lt;/p&gt;
&lt;h4 id=&#34;4-总结&#34;&gt;4. 总结&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;go build -o &#39;coupon&#39;

./coupon &amp;gt; coupon.log &amp;amp;

$GOPATH/bin/pprof -http=:9000 coupon http://localhost:8000/debug/pprof/profile

&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;reference&#34;&gt;reference&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://artem.krylysov.com/blog/2017/03/13/profiling-and-optimizing-go-web-applications/&#34;&gt;http://artem.krylysov.com/blog/2017/03/13/profiling-and-optimizing-go-web-applications/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/google/pprof/blob/master/doc/pprof.md&#34;&gt;https://github.com/google/pprof/blob/master/doc/pprof.md&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;性能测试应该怎么做？ &lt;a href=&#34;https://coolshell.cn/articles/17381.html&#34;&gt;https://coolshell.cn/articles/17381.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
- https://xujiahua.github.io/posts/20181017-perf-testing/ - </description>
        </item>
    
    
    
    
  </channel>
</rss> 